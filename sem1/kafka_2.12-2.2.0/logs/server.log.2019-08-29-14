[2019-08-29 12:11:26,663] INFO Processed session termination for sessionid: 0x100004b9a740001 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 12:11:26,675] INFO Closed socket connection for client /127.0.0.1:48986 which had sessionid 0x100004b9a740001 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-08-29 14:20:46,942] INFO Reading configuration from: ./kafka_2.12-2.2.0/config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-08-29 14:20:46,950] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-08-29 14:20:46,950] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-08-29 14:20:46,950] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-08-29 14:20:46,951] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-08-29 14:20:46,965] INFO Reading configuration from: ./kafka_2.12-2.2.0/config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-08-29 14:20:46,966] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-08-29 14:20:46,974] INFO Server environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,975] INFO Server environment:host.name=Christian-Inspiron.localdomain (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,975] INFO Server environment:java.version=11.0.4 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,975] INFO Server environment:java.vendor=Ubuntu (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,976] INFO Server environment:java.home=/usr/lib/jvm/java-11-openjdk-amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,977] INFO Server environment:java.class.path=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/activation-1.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/aopalliance-repackaged-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/argparse4j-0.7.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/audience-annotations-0.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/commons-lang3-3.8.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-api-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-basic-auth-extension-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-file-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-json-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-runtime-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-transforms-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/guava-20.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-api-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-locator-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-utils-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-core-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-databind-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-datatype-jdk8-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-base-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-json-provider-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-module-jaxb-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javassist-3.22.0-CR2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.annotation-api-1.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.servlet-api-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jaxb-api-2.3.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-client-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-common-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-core-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-hk2-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-media-jaxb-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-server-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-client-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-continuation-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-http-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-io-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-security-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-server-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlet-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlets-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-util-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jopt-simple-5.0.4.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-clients-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-log4j-appender-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-examples-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-scala_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-test-utils-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-tools-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0-sources.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/log4j-1.2.17.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/lz4-java-1.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/maven-artifact-3.6.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/metrics-core-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/osgi-resource-locator-1.0.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/plexus-utils-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/reflections-0.9.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/rocksdbjni-5.15.10.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-library-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-logging_2.12-3.9.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-reflect-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-api-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-log4j12-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/snappy-java-1.1.7.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/validation-api-1.1.0.Final.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zkclient-0.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zookeeper-3.4.13.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zstd-jni-1.3.8-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,978] INFO Server environment:java.library.path=/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,979] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,980] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,980] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,981] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,983] INFO Server environment:os.version=4.4.0-18362-Microsoft (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,986] INFO Server environment:user.name=christian (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,987] INFO Server environment:user.home=/home/christian (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,988] INFO Server environment:user.dir=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,996] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,997] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:46,998] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:20:47,011] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-08-29 14:20:47,019] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-08-29 14:21:34,267] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-08-29 14:21:34,712] INFO starting (kafka.server.KafkaServer)
[2019-08-29 14:21:34,713] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-08-29 14:21:34,739] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:21:34,824] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:21:34,824] INFO Client environment:host.name=Christian-Inspiron.localdomain (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:21:34,824] INFO Client environment:java.version=11.0.4 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:21:34,824] INFO Client environment:java.vendor=Ubuntu (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:21:34,825] INFO Client environment:java.home=/usr/lib/jvm/java-11-openjdk-amd64 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:21:34,825] INFO Client environment:java.class.path=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/activation-1.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/aopalliance-repackaged-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/argparse4j-0.7.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/audience-annotations-0.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/commons-lang3-3.8.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-api-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-basic-auth-extension-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-file-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-json-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-runtime-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-transforms-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/guava-20.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-api-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-locator-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-utils-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-core-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-databind-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-datatype-jdk8-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-base-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-json-provider-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-module-jaxb-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javassist-3.22.0-CR2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.annotation-api-1.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.servlet-api-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jaxb-api-2.3.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-client-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-common-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-core-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-hk2-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-media-jaxb-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-server-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-client-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-continuation-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-http-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-io-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-security-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-server-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlet-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlets-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-util-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jopt-simple-5.0.4.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-clients-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-log4j-appender-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-examples-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-scala_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-test-utils-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-tools-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0-sources.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/log4j-1.2.17.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/lz4-java-1.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/maven-artifact-3.6.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/metrics-core-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/osgi-resource-locator-1.0.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/plexus-utils-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/reflections-0.9.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/rocksdbjni-5.15.10.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-library-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-logging_2.12-3.9.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-reflect-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-api-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-log4j12-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/snappy-java-1.1.7.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/validation-api-1.1.0.Final.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zkclient-0.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zookeeper-3.4.13.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zstd-jni-1.3.8-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:21:34,830] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:21:34,831] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:21:34,831] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:21:34,832] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:21:34,833] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:21:34,834] INFO Client environment:os.version=4.4.0-18362-Microsoft (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:21:34,834] INFO Client environment:user.name=christian (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:21:34,835] INFO Client environment:user.home=/home/christian (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:21:34,836] INFO Client environment:user.dir=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:21:34,839] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@50378a4 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:21:34,857] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:21:34,857] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:21:34,866] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:21:34,867] INFO Accepted socket connection from /127.0.0.1:57597 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-08-29 14:21:34,878] INFO Client attempting to establish new session at /127.0.0.1:57597 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:21:34,880] INFO Creating new log file: log.b58 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-08-29 14:21:34,896] INFO Established session 0x1000003d3af0000 with negotiated timeout 6000 for client /127.0.0.1:57597 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:21:34,898] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1000003d3af0000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:21:34,906] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:21:34,953] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0000 type:create cxid:0x1 zxid:0xb59 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:21:34,969] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0000 type:create cxid:0x2 zxid:0xb5a txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:21:34,973] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0000 type:create cxid:0x3 zxid:0xb5b txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:21:34,980] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0000 type:create cxid:0x4 zxid:0xb5c txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:21:34,983] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0000 type:create cxid:0x5 zxid:0xb5d txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:21:34,988] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0000 type:create cxid:0x6 zxid:0xb5e txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:21:34,992] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0000 type:create cxid:0x7 zxid:0xb5f txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:21:34,998] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0000 type:create cxid:0x8 zxid:0xb60 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:21:35,002] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0000 type:create cxid:0x9 zxid:0xb61 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:21:35,006] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0000 type:create cxid:0xa zxid:0xb62 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:21:35,013] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0000 type:create cxid:0xb zxid:0xb63 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:21:35,017] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0000 type:create cxid:0xc zxid:0xb64 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:21:35,020] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0000 type:create cxid:0xd zxid:0xb65 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:21:35,182] INFO Cluster ID = eSotZ3p7R9WkH6FL3yanHA (kafka.server.KafkaServer)
[2019-08-29 14:21:35,244] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-08-29 14:21:35,314] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-08-29 14:21:35,354] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:21:35,354] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:21:35,356] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:21:35,390] INFO Loading logs. (kafka.log.LogManager)
[2019-08-29 14:21:35,473] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,503] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 75 ms (kafka.log.Log)
[2019-08-29 14:21:35,527] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,528] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-08-29 14:21:35,544] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,545] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-08-29 14:21:35,564] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,565] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:21:35,578] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,579] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:21:35,589] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,589] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:21:35,603] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,604] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:21:35,619] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,620] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:21:35,635] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,636] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:21:35,650] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,650] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-08-29 14:21:35,662] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,663] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:21:35,672] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,672] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:21:35,686] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,686] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-08-29 14:21:35,703] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,705] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-08-29 14:21:35,726] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,728] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 19 ms (kafka.log.Log)
[2019-08-29 14:21:35,746] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,747] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-08-29 14:21:35,759] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,760] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:21:35,772] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,772] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:21:35,786] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,788] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:21:35,806] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,808] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[2019-08-29 14:21:35,824] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,825] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-08-29 14:21:35,845] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,847] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-08-29 14:21:35,864] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,864] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:21:35,880] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,881] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-08-29 14:21:35,900] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,902] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[2019-08-29 14:21:35,917] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,918] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-08-29 14:21:35,930] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,931] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-08-29 14:21:35,943] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,943] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:21:35,955] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,955] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:21:35,968] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,969] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-08-29 14:21:35,980] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,980] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:21:35,992] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:35,992] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:21:36,004] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,005] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:21:36,018] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,018] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-08-29 14:21:36,027] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,028] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-08-29 14:21:36,039] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,040] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:21:36,052] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,053] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-08-29 14:21:36,067] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,068] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:21:36,079] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,080] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:21:36,092] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,093] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-08-29 14:21:36,106] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,107] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-08-29 14:21:36,136] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,138] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 28 ms (kafka.log.Log)
[2019-08-29 14:21:36,156] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,157] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-08-29 14:21:36,172] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,173] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-08-29 14:21:36,192] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,194] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 19 ms (kafka.log.Log)
[2019-08-29 14:21:36,206] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,207] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:21:36,218] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,218] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-08-29 14:21:36,229] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,230] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:21:36,245] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,246] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-08-29 14:21:36,264] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,266] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[2019-08-29 14:21:36,282] INFO [Log partition=alerts-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,282] INFO [Log partition=alerts-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:21:36,288] WARN [Log partition=tdn-systrk-0, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/tdn-systrk-0/00000000000000000000.log due to Corrupt time index found, time index file (/tmp/kafka-logs/tdn-systrk-0/00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1559707594268}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-08-29 14:21:36,289] INFO [Log partition=tdn-systrk-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:21:36,589] ERROR Error while loading log dir /tmp/kafka-logs (kafka.log.LogManager)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.LogSegment.recover(LogSegment.scala:377)
	at kafka.log.Log.recoverSegment(Log.scala:500)
	at kafka.log.Log.$anonfun$loadSegmentFiles$3(Log.scala:482)
	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:792)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:791)
	at kafka.log.Log.loadSegmentFiles(Log.scala:454)
	at kafka.log.Log.$anonfun$loadSegments$1(Log.scala:565)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2024)
	at kafka.log.Log.loadSegments(Log.scala:559)
	at kafka.log.Log.<init>(Log.scala:292)
	at kafka.log.Log$.apply(Log.scala:2158)
	at kafka.log.LogManager.loadLog(LogManager.scala:275)
	at kafka.log.LogManager.$anonfun$loadLogs$12(LogManager.scala:345)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:21:36,596] ERROR Error while deleting the clean shutdown file in dir /tmp/kafka-logs (kafka.server.LogDirFailureChannel)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.LogSegment.recover(LogSegment.scala:377)
	at kafka.log.Log.recoverSegment(Log.scala:500)
	at kafka.log.Log.$anonfun$loadSegmentFiles$3(Log.scala:482)
	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:792)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:791)
	at kafka.log.Log.loadSegmentFiles(Log.scala:454)
	at kafka.log.Log.$anonfun$loadSegments$1(Log.scala:565)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2024)
	at kafka.log.Log.loadSegments(Log.scala:559)
	at kafka.log.Log.<init>(Log.scala:292)
	at kafka.log.Log$.apply(Log.scala:2158)
	at kafka.log.LogManager.loadLog(LogManager.scala:275)
	at kafka.log.LogManager.$anonfun$loadLogs$12(LogManager.scala:345)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:21:36,599] INFO Logs loading complete in 1208 ms. (kafka.log.LogManager)
[2019-08-29 14:21:36,607] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-08-29 14:21:36,608] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-08-29 14:21:36,907] INFO Awaiting socket connections on slocalhost:9092. (kafka.network.Acceptor)
[2019-08-29 14:21:36,936] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(localhost,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-08-29 14:21:36,938] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-08-29 14:21:36,964] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:21:36,965] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:21:36,966] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:21:36,967] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:21:36,980] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-08-29 14:21:36,980] INFO [ReplicaManager broker=0] Stopping serving replicas in dir /tmp/kafka-logs (kafka.server.ReplicaManager)
[2019-08-29 14:21:36,983] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set() (kafka.server.ReplicaFetcherManager)
[2019-08-29 14:21:36,984] INFO [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set() (kafka.server.ReplicaAlterLogDirsManager)
[2019-08-29 14:21:36,991] INFO [ReplicaManager broker=0] Broker 0 stopped fetcher for partitions  and stopped moving logs for partitions  because they are in the failed log directory /tmp/kafka-logs. (kafka.server.ReplicaManager)
[2019-08-29 14:21:36,992] INFO Stopping serving logs in dir /tmp/kafka-logs (kafka.log.LogManager)
[2019-08-29 14:21:36,996] ERROR Shutdown broker because all log dirs in /tmp/kafka-logs have failed (kafka.log.LogManager)
[2019-08-29 14:21:37,349] WARN Unable to read additional data from client sessionid 0x1000003d3af0000, likely client has closed socket (org.apache.zookeeper.server.NIOServerCnxn)
[2019-08-29 14:21:37,351] INFO Closed socket connection for client /127.0.0.1:57597 which had sessionid 0x1000003d3af0000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-08-29 14:21:45,357] INFO Expiring session 0x1000003d3af0000, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:21:45,363] INFO Processed session termination for sessionid: 0x1000003d3af0000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:22:00,725] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-08-29 14:22:01,100] INFO starting (kafka.server.KafkaServer)
[2019-08-29 14:22:01,102] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-08-29 14:22:01,119] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:22:01,125] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:22:01,125] INFO Client environment:host.name=Christian-Inspiron.localdomain (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:22:01,125] INFO Client environment:java.version=11.0.4 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:22:01,126] INFO Client environment:java.vendor=Ubuntu (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:22:01,126] INFO Client environment:java.home=/usr/lib/jvm/java-11-openjdk-amd64 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:22:01,126] INFO Client environment:java.class.path=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/activation-1.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/aopalliance-repackaged-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/argparse4j-0.7.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/audience-annotations-0.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/commons-lang3-3.8.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-api-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-basic-auth-extension-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-file-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-json-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-runtime-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-transforms-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/guava-20.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-api-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-locator-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-utils-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-core-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-databind-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-datatype-jdk8-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-base-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-json-provider-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-module-jaxb-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javassist-3.22.0-CR2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.annotation-api-1.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.servlet-api-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jaxb-api-2.3.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-client-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-common-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-core-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-hk2-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-media-jaxb-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-server-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-client-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-continuation-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-http-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-io-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-security-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-server-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlet-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlets-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-util-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jopt-simple-5.0.4.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-clients-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-log4j-appender-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-examples-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-scala_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-test-utils-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-tools-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0-sources.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/log4j-1.2.17.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/lz4-java-1.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/maven-artifact-3.6.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/metrics-core-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/osgi-resource-locator-1.0.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/plexus-utils-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/reflections-0.9.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/rocksdbjni-5.15.10.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-library-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-logging_2.12-3.9.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-reflect-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-api-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-log4j12-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/snappy-java-1.1.7.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/validation-api-1.1.0.Final.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zkclient-0.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zookeeper-3.4.13.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zstd-jni-1.3.8-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:22:01,128] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:22:01,128] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:22:01,128] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:22:01,128] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:22:01,128] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:22:01,128] INFO Client environment:os.version=4.4.0-18362-Microsoft (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:22:01,128] INFO Client environment:user.name=christian (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:22:01,128] INFO Client environment:user.home=/home/christian (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:22:01,130] INFO Client environment:user.dir=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:22:01,131] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@50378a4 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:22:01,142] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:22:01,143] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:22:01,148] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:22:01,148] INFO Accepted socket connection from /127.0.0.1:57612 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-08-29 14:22:01,153] INFO Client attempting to establish new session at /127.0.0.1:57612 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:22:01,159] INFO Established session 0x1000003d3af0001 with negotiated timeout 6000 for client /127.0.0.1:57612 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:22:01,161] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1000003d3af0001, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:22:01,164] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:22:01,191] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0001 type:create cxid:0x1 zxid:0xb68 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:22:01,202] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0001 type:create cxid:0x2 zxid:0xb69 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:22:01,206] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0001 type:create cxid:0x3 zxid:0xb6a txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:22:01,210] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0001 type:create cxid:0x4 zxid:0xb6b txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:22:01,213] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0001 type:create cxid:0x5 zxid:0xb6c txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:22:01,217] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0001 type:create cxid:0x6 zxid:0xb6d txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:22:01,221] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0001 type:create cxid:0x7 zxid:0xb6e txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:22:01,225] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0001 type:create cxid:0x8 zxid:0xb6f txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:22:01,228] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0001 type:create cxid:0x9 zxid:0xb70 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:22:01,231] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0001 type:create cxid:0xa zxid:0xb71 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:22:01,235] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0001 type:create cxid:0xb zxid:0xb72 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:22:01,241] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0001 type:create cxid:0xc zxid:0xb73 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:22:01,244] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0001 type:create cxid:0xd zxid:0xb74 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:22:01,386] INFO Cluster ID = eSotZ3p7R9WkH6FL3yanHA (kafka.server.KafkaServer)
[2019-08-29 14:22:01,389] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-08-29 14:22:01,429] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-08-29 14:22:01,447] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-08-29 14:22:01,477] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:22:01,477] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:22:01,479] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:22:01,501] INFO Log directory /tmp/kafka-logs not found, creating it. (kafka.log.LogManager)
[2019-08-29 14:22:01,515] INFO Loading logs. (kafka.log.LogManager)
[2019-08-29 14:22:01,528] INFO Logs loading complete in 12 ms. (kafka.log.LogManager)
[2019-08-29 14:22:01,541] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-08-29 14:22:01,544] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-08-29 14:22:01,854] INFO Awaiting socket connections on slocalhost:9092. (kafka.network.Acceptor)
[2019-08-29 14:22:01,879] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(localhost,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-08-29 14:22:01,881] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-08-29 14:22:01,899] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:22:01,900] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:22:01,901] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:22:01,902] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:22:01,913] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-08-29 14:22:01,950] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-08-29 14:22:01,974] INFO Stat of the created znode at /brokers/ids/0 is: 2933,2933,1567054321959,1567054321959,1,0,0,72057610474291201,188,0,2933
 (kafka.zk.KafkaZkClient)
[2019-08-29 14:22:01,976] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(localhost,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 2933 (kafka.zk.KafkaZkClient)
[2019-08-29 14:22:01,978] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-08-29 14:22:02,040] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:22:02,044] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:22:02,045] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:22:02,067] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:22:02,069] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:22:02,074] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:02,086] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:5000,blockEndProducerId:5999) by writing to Zk with path version 6 (kafka.coordinator.transaction.ProducerIdManager)
[2019-08-29 14:22:02,109] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-08-29 14:22:02,112] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-08-29 14:22:02,112] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-08-29 14:22:02,153] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-08-29 14:22:02,179] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2019-08-29 14:22:02,190] INFO Kafka version: 2.2.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-08-29 14:22:02,190] INFO Kafka commitId: 05fcfde8f69b0349 (org.apache.kafka.common.utils.AppInfoParser)
[2019-08-29 14:22:02,192] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2019-08-29 14:22:02,217] INFO Got user-level KeeperException when processing sessionid:0x1000003d3af0001 type:multi cxid:0x6b zxid:0xb78 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:22:02,264] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, alerts-0, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, tdn-systrk-0, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-08-29 14:22:02,306] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,312] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 31 ms (kafka.log.Log)
[2019-08-29 14:22:02,314] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,315] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2019-08-29 14:22:02,316] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,319] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,349] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,350] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,351] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,352] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2019-08-29 14:22:02,352] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,352] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,365] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,366] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,367] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,367] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2019-08-29 14:22:02,368] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,368] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,380] INFO [Log partition=alerts-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,382] INFO [Log partition=alerts-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:22:02,382] INFO Created log for partition alerts-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,383] INFO [Partition alerts-0 broker=0] No checkpointed highwatermark is found for partition alerts-0 (kafka.cluster.Partition)
[2019-08-29 14:22:02,383] INFO Replica loaded for partition alerts-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,383] INFO [Partition alerts-0 broker=0] alerts-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,397] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,398] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,399] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,400] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2019-08-29 14:22:02,400] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,401] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,413] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,414] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:22:02,415] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,415] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2019-08-29 14:22:02,415] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,416] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,429] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,430] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,430] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,431] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2019-08-29 14:22:02,431] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,431] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,445] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,446] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:22:02,446] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,447] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2019-08-29 14:22:02,447] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,447] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,460] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,461] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,462] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,462] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2019-08-29 14:22:02,462] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,462] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,482] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,484] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:22:02,484] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,485] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2019-08-29 14:22:02,485] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,485] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,498] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,499] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,500] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,500] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2019-08-29 14:22:02,500] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,501] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,512] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,514] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,514] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,515] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,515] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,515] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,534] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,536] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:22:02,538] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,538] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2019-08-29 14:22:02,539] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,539] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,552] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,552] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:22:02,553] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,554] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2019-08-29 14:22:02,555] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,555] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,568] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,569] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,569] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,569] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2019-08-29 14:22:02,570] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,570] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,582] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,583] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,584] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,584] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2019-08-29 14:22:02,584] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,585] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,597] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,597] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:22:02,598] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,598] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2019-08-29 14:22:02,599] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,599] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,612] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,613] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,614] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,614] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2019-08-29 14:22:02,614] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,615] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,628] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,629] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,630] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,630] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2019-08-29 14:22:02,631] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,631] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,644] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,645] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,646] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,646] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2019-08-29 14:22:02,646] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,647] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,661] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,662] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,662] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,663] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2019-08-29 14:22:02,663] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,663] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,676] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,677] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,677] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,678] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2019-08-29 14:22:02,678] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,678] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,697] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,698] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:22:02,699] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,700] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2019-08-29 14:22:02,700] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,700] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,713] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,713] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:22:02,714] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,714] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2019-08-29 14:22:02,714] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,715] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,727] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,728] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,728] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,729] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2019-08-29 14:22:02,729] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,729] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,749] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,751] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:22:02,752] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,752] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2019-08-29 14:22:02,752] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,753] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,773] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,775] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:22:02,776] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,777] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2019-08-29 14:22:02,777] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,777] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,798] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,799] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:22:02,800] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,801] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2019-08-29 14:22:02,801] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,801] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,814] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,815] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,815] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,816] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2019-08-29 14:22:02,816] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,816] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,836] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,838] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:22:02,839] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,839] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2019-08-29 14:22:02,840] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,840] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,853] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,854] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,855] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,855] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2019-08-29 14:22:02,855] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,856] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,877] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,879] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:22:02,879] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,880] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2019-08-29 14:22:02,880] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,881] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,894] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,895] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,895] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,896] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2019-08-29 14:22:02,896] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,896] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,909] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,910] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,910] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,911] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2019-08-29 14:22:02,911] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,911] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,930] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,932] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:22:02,932] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,933] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2019-08-29 14:22:02,933] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,933] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,946] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,947] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,947] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,948] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2019-08-29 14:22:02,948] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,948] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,960] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,961] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,962] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,962] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2019-08-29 14:22:02,962] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,963] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,976] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,976] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:22:02,977] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,977] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2019-08-29 14:22:02,977] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,978] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:02,990] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:02,990] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:02,991] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:02,992] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2019-08-29 14:22:02,992] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:02,992] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:03,005] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:03,006] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:03,007] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:03,007] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2019-08-29 14:22:03,008] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:03,008] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:03,019] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:03,020] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:03,021] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:03,021] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2019-08-29 14:22:03,021] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:03,022] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:03,042] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:03,043] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:22:03,044] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:03,045] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2019-08-29 14:22:03,045] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:03,045] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:03,066] INFO [Log partition=tdn-systrk-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:03,068] INFO [Log partition=tdn-systrk-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:22:03,069] INFO Created log for partition tdn-systrk-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:03,069] INFO [Partition tdn-systrk-0 broker=0] No checkpointed highwatermark is found for partition tdn-systrk-0 (kafka.cluster.Partition)
[2019-08-29 14:22:03,069] INFO Replica loaded for partition tdn-systrk-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:03,070] INFO [Partition tdn-systrk-0 broker=0] tdn-systrk-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:03,082] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:03,083] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:03,084] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:03,084] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2019-08-29 14:22:03,084] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:03,084] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:03,096] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:03,098] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:22:03,098] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:03,098] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2019-08-29 14:22:03,099] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:03,099] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:03,119] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:03,121] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:22:03,122] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:03,123] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2019-08-29 14:22:03,123] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:03,123] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:03,136] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:03,137] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:03,138] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:03,140] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2019-08-29 14:22:03,140] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:03,141] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:03,153] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:03,153] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:22:03,155] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:03,156] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2019-08-29 14:22:03,156] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:03,156] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:03,169] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:03,171] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:22:03,172] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:03,172] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2019-08-29 14:22:03,173] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:03,173] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:03,185] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:03,186] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:03,186] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:03,187] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2019-08-29 14:22:03,187] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:03,188] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:03,201] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:03,202] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:22:03,203] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:03,203] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2019-08-29 14:22:03,203] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:03,203] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:03,217] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:22:03,217] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:22:03,218] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:22:03,219] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2019-08-29 14:22:03,219] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:22:03,219] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:22:03,232] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,234] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,235] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,235] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,235] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,235] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,235] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,236] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,236] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,236] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,236] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,237] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,237] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,237] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,238] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,238] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,238] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,238] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,239] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,239] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,240] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,240] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,240] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,240] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,240] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,241] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,241] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,241] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,241] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,241] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,241] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,241] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,242] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,242] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,244] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,244] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,244] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,244] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,246] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,246] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,246] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,246] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,246] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,246] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,248] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 14 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,249] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,250] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,250] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,250] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,250] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,251] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,251] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,251] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,251] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,252] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,255] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,256] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,256] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,257] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,257] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,257] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,258] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,258] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,258] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,259] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,259] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,260] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,260] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,260] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,261] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,261] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,261] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,261] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,261] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,262] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,262] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,262] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,262] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,262] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,263] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,263] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,265] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,265] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,265] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,265] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,265] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,266] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,266] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,266] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,266] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,266] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,267] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,267] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:22:03,267] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:26:45,658] INFO Unable to read additional data from server sessionid 0x1000003d3af0001, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:26:47,704] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:26:49,711] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:26:49,744] INFO Reading configuration from: ./kafka_2.12-2.2.0/config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-08-29 14:26:49,747] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-08-29 14:26:49,747] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-08-29 14:26:49,747] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-08-29 14:26:49,748] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-08-29 14:26:49,770] INFO Reading configuration from: ./kafka_2.12-2.2.0/config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-08-29 14:26:49,772] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-08-29 14:26:49,781] INFO Server environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,782] INFO Server environment:host.name=Christian-Inspiron.localdomain (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,782] INFO Server environment:java.version=11.0.4 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,782] INFO Server environment:java.vendor=Ubuntu (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,782] INFO Server environment:java.home=/usr/lib/jvm/java-11-openjdk-amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,782] INFO Server environment:java.class.path=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/activation-1.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/aopalliance-repackaged-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/argparse4j-0.7.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/audience-annotations-0.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/commons-lang3-3.8.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-api-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-basic-auth-extension-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-file-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-json-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-runtime-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-transforms-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/guava-20.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-api-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-locator-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-utils-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-core-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-databind-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-datatype-jdk8-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-base-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-json-provider-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-module-jaxb-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javassist-3.22.0-CR2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.annotation-api-1.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.servlet-api-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jaxb-api-2.3.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-client-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-common-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-core-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-hk2-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-media-jaxb-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-server-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-client-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-continuation-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-http-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-io-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-security-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-server-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlet-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlets-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-util-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jopt-simple-5.0.4.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-clients-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-log4j-appender-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-examples-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-scala_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-test-utils-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-tools-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0-sources.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/log4j-1.2.17.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/lz4-java-1.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/maven-artifact-3.6.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/metrics-core-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/osgi-resource-locator-1.0.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/plexus-utils-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/reflections-0.9.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/rocksdbjni-5.15.10.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-library-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-logging_2.12-3.9.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-reflect-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-api-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-log4j12-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/snappy-java-1.1.7.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/validation-api-1.1.0.Final.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zkclient-0.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zookeeper-3.4.13.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zstd-jni-1.3.8-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,787] INFO Server environment:java.library.path=/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,789] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,799] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,800] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,803] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,805] INFO Server environment:os.version=4.4.0-18362-Microsoft (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,807] INFO Server environment:user.name=christian (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,811] INFO Server environment:user.home=/home/christian (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,816] INFO Server environment:user.dir=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,816] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:26:49,831] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,832] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,840] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:49,860] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-08-29 14:26:49,875] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-08-29 14:26:51,157] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:26:51,158] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:26:51,160] INFO Accepted socket connection from /127.0.0.1:57764 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-08-29 14:26:51,166] INFO Client attempting to renew session 0x1000003d3af0001 at /127.0.0.1:57764 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:51,175] INFO Established session 0x1000003d3af0001 with negotiated timeout 6000 for client /127.0.0.1:57764 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:26:51,175] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1000003d3af0001, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:26:51,176] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:26:51,184] INFO Creating new log file: log.baa (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-08-29 14:28:43,343] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:28:43,346] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2019-08-29 14:28:43,347] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)
[2019-08-29 14:28:43,380] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2019-08-29 14:28:43,386] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-08-29 14:28:43,390] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-08-29 14:28:43,390] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-08-29 14:28:43,392] INFO [SocketServer brokerId=0] Stopping socket server request processors (kafka.network.SocketServer)
[2019-08-29 14:28:43,416] INFO [SocketServer brokerId=0] Stopped socket server request processors (kafka.network.SocketServer)
[2019-08-29 14:28:43,418] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool)
[2019-08-29 14:28:43,423] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2019-08-29 14:28:43,431] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis)
[2019-08-29 14:28:43,434] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:43,615] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:43,615] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:43,619] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-08-29 14:28:43,622] INFO [ProducerId Manager 0]: Shutdown complete: last producerId assigned 5000 (kafka.coordinator.transaction.ProducerIdManager)
[2019-08-29 14:28:43,626] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2019-08-29 14:28:43,628] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-08-29 14:28:43,629] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-08-29 14:28:43,629] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-08-29 14:28:43,631] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-08-29 14:28:43,648] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:28:43,649] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:43,813] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:43,814] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:43,816] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:44,016] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:44,016] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:44,018] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:28:44,020] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)
[2019-08-29 14:28:44,021] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-08-29 14:28:44,024] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-08-29 14:28:44,024] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-08-29 14:28:44,033] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)
[2019-08-29 14:28:44,034] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)
[2019-08-29 14:28:44,035] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2019-08-29 14:28:44,036] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2019-08-29 14:28:44,042] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:44,071] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:44,071] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:44,076] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:44,260] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:44,260] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:44,262] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:44,463] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:44,463] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:44,464] INFO [ExpirationReaper-0-ElectPreferredLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:44,664] INFO [ExpirationReaper-0-ElectPreferredLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:44,664] INFO [ExpirationReaper-0-ElectPreferredLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:44,678] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)
[2019-08-29 14:28:44,679] INFO Shutting down. (kafka.log.LogManager)
[2019-08-29 14:28:44,706] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:44,802] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:44,919] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,017] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,118] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,129] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,139] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,150] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,158] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,168] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,188] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,197] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,204] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,213] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,221] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,234] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,248] INFO [ProducerStateManager partition=tdn-systrk-0] Writing producer snapshot at offset 1845 (kafka.log.ProducerStateManager)
[2019-08-29 14:28:45,250] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,256] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,274] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,284] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,293] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,301] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,309] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,324] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,340] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,352] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,360] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,374] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,390] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,394] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,409] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,422] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,439] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,444] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,459] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,474] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,492] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,503] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,512] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,524] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,541] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,553] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,561] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,573] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,590] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,602] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,609] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,623] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,640] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,650] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,658] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,668] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,684] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,690] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,702] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,717] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,725] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,735] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,751] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,757] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,770] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,784] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,792] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,802] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,817] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,823] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,839] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,850] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,859] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,868] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,884] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,899] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,908] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,918] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,934] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,940] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,957] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,969] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,991] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:45,995] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,010] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,023] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,040] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,044] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,062] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,075] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,089] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,107] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,123] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,127] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,163] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,180] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,196] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,199] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,216] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,235] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,244] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,256] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,278] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,294] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,309] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,319] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,337] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,351] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:46,396] INFO Shutdown complete. (kafka.log.LogManager)
[2019-08-29 14:28:46,405] INFO [ZooKeeperClient] Closing. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:28:46,406] INFO Processed session termination for sessionid: 0x1000003d3af0001 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:28:46,415] INFO Session: 0x1000003d3af0001 closed (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:46,416] INFO Closed socket connection for client /127.0.0.1:57764 which had sessionid 0x1000003d3af0001 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-08-29 14:28:46,416] INFO EventThread shut down for session: 0x1000003d3af0001 (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:28:46,416] INFO [ZooKeeperClient] Closed. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:28:46,421] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:28:46,641] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:28:46,641] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:28:46,643] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:28:47,641] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:28:47,641] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:28:47,643] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:28:48,643] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:28:48,643] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:28:48,646] INFO [SocketServer brokerId=0] Shutting down socket server (kafka.network.SocketServer)
[2019-08-29 14:28:48,667] INFO [SocketServer brokerId=0] Shutdown completed (kafka.network.SocketServer)
[2019-08-29 14:28:48,673] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)
[2019-08-29 14:28:54,971] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-08-29 14:28:55,413] INFO starting (kafka.server.KafkaServer)
[2019-08-29 14:28:55,415] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-08-29 14:28:55,438] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:28:55,443] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:55,443] INFO Client environment:host.name=Christian-Inspiron.localdomain (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:55,443] INFO Client environment:java.version=11.0.4 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:55,443] INFO Client environment:java.vendor=Ubuntu (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:55,443] INFO Client environment:java.home=/usr/lib/jvm/java-11-openjdk-amd64 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:55,444] INFO Client environment:java.class.path=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/activation-1.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/aopalliance-repackaged-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/argparse4j-0.7.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/audience-annotations-0.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/commons-lang3-3.8.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-api-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-basic-auth-extension-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-file-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-json-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-runtime-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-transforms-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/guava-20.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-api-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-locator-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-utils-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-core-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-databind-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-datatype-jdk8-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-base-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-json-provider-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-module-jaxb-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javassist-3.22.0-CR2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.annotation-api-1.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.servlet-api-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jaxb-api-2.3.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-client-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-common-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-core-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-hk2-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-media-jaxb-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-server-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-client-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-continuation-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-http-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-io-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-security-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-server-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlet-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlets-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-util-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jopt-simple-5.0.4.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-clients-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-log4j-appender-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-examples-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-scala_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-test-utils-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-tools-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0-sources.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/log4j-1.2.17.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/lz4-java-1.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/maven-artifact-3.6.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/metrics-core-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/osgi-resource-locator-1.0.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/plexus-utils-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/reflections-0.9.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/rocksdbjni-5.15.10.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-library-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-logging_2.12-3.9.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-reflect-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-api-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-log4j12-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/snappy-java-1.1.7.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/validation-api-1.1.0.Final.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zkclient-0.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zookeeper-3.4.13.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zstd-jni-1.3.8-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:55,445] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:55,445] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:55,445] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:55,445] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:55,445] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:55,445] INFO Client environment:os.version=4.4.0-18362-Microsoft (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:55,445] INFO Client environment:user.name=christian (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:55,446] INFO Client environment:user.home=/home/christian (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:55,446] INFO Client environment:user.dir=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:55,447] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@50378a4 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:28:55,463] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:28:55,464] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:28:55,474] INFO Accepted socket connection from /127.0.0.1:57896 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-08-29 14:28:55,474] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:28:55,484] INFO Client attempting to establish new session at /127.0.0.1:57896 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:28:55,497] INFO Established session 0x10000095d590000 with negotiated timeout 6000 for client /127.0.0.1:57896 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:28:55,499] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000095d590000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:28:55,509] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:28:55,560] INFO Got user-level KeeperException when processing sessionid:0x10000095d590000 type:create cxid:0x1 zxid:0xbc3 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:28:55,589] INFO Got user-level KeeperException when processing sessionid:0x10000095d590000 type:create cxid:0x2 zxid:0xbc4 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:28:55,597] INFO Got user-level KeeperException when processing sessionid:0x10000095d590000 type:create cxid:0x3 zxid:0xbc5 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:28:55,601] INFO Got user-level KeeperException when processing sessionid:0x10000095d590000 type:create cxid:0x4 zxid:0xbc6 txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:28:55,610] INFO Got user-level KeeperException when processing sessionid:0x10000095d590000 type:create cxid:0x5 zxid:0xbc7 txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:28:55,618] INFO Got user-level KeeperException when processing sessionid:0x10000095d590000 type:create cxid:0x6 zxid:0xbc8 txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:28:55,624] INFO Got user-level KeeperException when processing sessionid:0x10000095d590000 type:create cxid:0x7 zxid:0xbc9 txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:28:55,631] INFO Got user-level KeeperException when processing sessionid:0x10000095d590000 type:create cxid:0x8 zxid:0xbca txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:28:55,635] INFO Got user-level KeeperException when processing sessionid:0x10000095d590000 type:create cxid:0x9 zxid:0xbcb txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:28:55,643] INFO Got user-level KeeperException when processing sessionid:0x10000095d590000 type:create cxid:0xa zxid:0xbcc txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:28:55,651] INFO Got user-level KeeperException when processing sessionid:0x10000095d590000 type:create cxid:0xb zxid:0xbcd txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:28:55,656] INFO Got user-level KeeperException when processing sessionid:0x10000095d590000 type:create cxid:0xc zxid:0xbce txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:28:55,663] INFO Got user-level KeeperException when processing sessionid:0x10000095d590000 type:create cxid:0xd zxid:0xbcf txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:28:55,827] INFO Cluster ID = eSotZ3p7R9WkH6FL3yanHA (kafka.server.KafkaServer)
[2019-08-29 14:28:55,881] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-08-29 14:28:55,910] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-08-29 14:28:55,963] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:28:55,963] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:28:55,966] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:28:56,007] INFO Loading logs. (kafka.log.LogManager)
[2019-08-29 14:28:56,094] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,126] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 73 ms (kafka.log.Log)
[2019-08-29 14:28:56,150] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,151] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:28:56,165] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,166] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-08-29 14:28:56,190] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,192] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[2019-08-29 14:28:56,208] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,209] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-08-29 14:28:56,225] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,226] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:28:56,237] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,240] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-08-29 14:28:56,260] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,261] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-08-29 14:28:56,279] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,279] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[2019-08-29 14:28:56,305] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,307] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 23 ms (kafka.log.Log)
[2019-08-29 14:28:56,324] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,324] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-08-29 14:28:56,337] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,342] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-08-29 14:28:56,357] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,358] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:28:56,375] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,376] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[2019-08-29 14:28:56,404] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,406] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 27 ms (kafka.log.Log)
[2019-08-29 14:28:56,423] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,424] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:28:56,434] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,438] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-08-29 14:28:56,458] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,459] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-08-29 14:28:56,474] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,475] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-08-29 14:28:56,489] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,489] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-08-29 14:28:56,509] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,510] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 19 ms (kafka.log.Log)
[2019-08-29 14:28:56,524] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,525] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-08-29 14:28:56,539] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,540] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:28:56,557] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,559] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[2019-08-29 14:28:56,577] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,579] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-08-29 14:28:56,597] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,598] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[2019-08-29 14:28:56,616] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,618] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-08-29 14:28:56,639] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,640] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[2019-08-29 14:28:56,656] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,657] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:28:56,673] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,674] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[2019-08-29 14:28:56,682] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,682] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:28:56,694] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,694] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:28:56,709] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,709] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:28:56,723] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,724] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:28:56,732] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,734] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-08-29 14:28:56,752] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,756] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-08-29 14:28:56,767] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,767] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:28:56,785] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,789] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[2019-08-29 14:28:56,803] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,806] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:28:56,823] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,824] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[2019-08-29 14:28:56,835] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,837] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-08-29 14:28:56,855] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,858] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[2019-08-29 14:28:56,874] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,874] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:28:56,885] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,889] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:28:56,905] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,906] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:28:56,923] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,924] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-08-29 14:28:56,934] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,935] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:28:56,946] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,946] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:28:56,968] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,972] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 24 ms (kafka.log.Log)
[2019-08-29 14:28:56,989] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:56,990] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[2019-08-29 14:28:57,000] INFO [Log partition=alerts-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:57,001] INFO [Log partition=alerts-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-08-29 14:28:57,009] WARN [Log partition=tdn-systrk-0, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/tdn-systrk-0/00000000000000000000.log due to Corrupt time index found, time index file (/tmp/kafka-logs/tdn-systrk-0/00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1567054471017}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-08-29 14:28:57,010] INFO [Log partition=tdn-systrk-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:28:57,079] ERROR Error while loading log dir /tmp/kafka-logs (kafka.log.LogManager)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.LogSegment.recover(LogSegment.scala:377)
	at kafka.log.Log.recoverSegment(Log.scala:500)
	at kafka.log.Log.$anonfun$loadSegmentFiles$3(Log.scala:482)
	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:792)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:791)
	at kafka.log.Log.loadSegmentFiles(Log.scala:454)
	at kafka.log.Log.$anonfun$loadSegments$1(Log.scala:565)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2024)
	at kafka.log.Log.loadSegments(Log.scala:559)
	at kafka.log.Log.<init>(Log.scala:292)
	at kafka.log.Log$.apply(Log.scala:2158)
	at kafka.log.LogManager.loadLog(LogManager.scala:275)
	at kafka.log.LogManager.$anonfun$loadLogs$12(LogManager.scala:345)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:57,084] ERROR Error while deleting the clean shutdown file in dir /tmp/kafka-logs (kafka.server.LogDirFailureChannel)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.LogSegment.recover(LogSegment.scala:377)
	at kafka.log.Log.recoverSegment(Log.scala:500)
	at kafka.log.Log.$anonfun$loadSegmentFiles$3(Log.scala:482)
	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:792)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:791)
	at kafka.log.Log.loadSegmentFiles(Log.scala:454)
	at kafka.log.Log.$anonfun$loadSegments$1(Log.scala:565)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2024)
	at kafka.log.Log.loadSegments(Log.scala:559)
	at kafka.log.Log.<init>(Log.scala:292)
	at kafka.log.Log$.apply(Log.scala:2158)
	at kafka.log.LogManager.loadLog(LogManager.scala:275)
	at kafka.log.LogManager.$anonfun$loadLogs$12(LogManager.scala:345)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:28:57,088] INFO Logs loading complete in 1077 ms. (kafka.log.LogManager)
[2019-08-29 14:28:57,098] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-08-29 14:28:57,099] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-08-29 14:28:57,409] INFO Awaiting socket connections on slocalhost:9092. (kafka.network.Acceptor)
[2019-08-29 14:28:57,447] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(localhost,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-08-29 14:28:57,449] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-08-29 14:28:57,475] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:57,477] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:57,478] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:57,479] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:28:57,493] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-08-29 14:28:57,494] INFO [ReplicaManager broker=0] Stopping serving replicas in dir /tmp/kafka-logs (kafka.server.ReplicaManager)
[2019-08-29 14:28:57,498] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set() (kafka.server.ReplicaFetcherManager)
[2019-08-29 14:28:57,502] INFO [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set() (kafka.server.ReplicaAlterLogDirsManager)
[2019-08-29 14:28:57,524] INFO [ReplicaManager broker=0] Broker 0 stopped fetcher for partitions  and stopped moving logs for partitions  because they are in the failed log directory /tmp/kafka-logs. (kafka.server.ReplicaManager)
[2019-08-29 14:28:57,525] INFO Stopping serving logs in dir /tmp/kafka-logs (kafka.log.LogManager)
[2019-08-29 14:28:57,528] ERROR Shutdown broker because all log dirs in /tmp/kafka-logs have failed (kafka.log.LogManager)
[2019-08-29 14:28:57,880] WARN Unable to read additional data from client sessionid 0x10000095d590000, likely client has closed socket (org.apache.zookeeper.server.NIOServerCnxn)
[2019-08-29 14:28:57,881] INFO Closed socket connection for client /127.0.0.1:57896 which had sessionid 0x10000095d590000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-08-29 14:29:06,357] INFO Expiring session 0x10000095d590000, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:29:06,358] INFO Processed session termination for sessionid: 0x10000095d590000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:29:06,492] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-08-29 14:29:06,916] INFO starting (kafka.server.KafkaServer)
[2019-08-29 14:29:06,917] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-08-29 14:29:06,942] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:29:06,946] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:29:06,947] INFO Client environment:host.name=Christian-Inspiron.localdomain (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:29:06,947] INFO Client environment:java.version=11.0.4 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:29:06,947] INFO Client environment:java.vendor=Ubuntu (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:29:06,947] INFO Client environment:java.home=/usr/lib/jvm/java-11-openjdk-amd64 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:29:06,947] INFO Client environment:java.class.path=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/activation-1.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/aopalliance-repackaged-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/argparse4j-0.7.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/audience-annotations-0.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/commons-lang3-3.8.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-api-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-basic-auth-extension-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-file-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-json-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-runtime-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-transforms-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/guava-20.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-api-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-locator-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-utils-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-core-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-databind-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-datatype-jdk8-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-base-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-json-provider-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-module-jaxb-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javassist-3.22.0-CR2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.annotation-api-1.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.servlet-api-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jaxb-api-2.3.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-client-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-common-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-core-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-hk2-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-media-jaxb-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-server-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-client-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-continuation-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-http-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-io-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-security-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-server-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlet-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlets-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-util-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jopt-simple-5.0.4.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-clients-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-log4j-appender-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-examples-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-scala_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-test-utils-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-tools-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0-sources.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/log4j-1.2.17.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/lz4-java-1.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/maven-artifact-3.6.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/metrics-core-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/osgi-resource-locator-1.0.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/plexus-utils-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/reflections-0.9.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/rocksdbjni-5.15.10.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-library-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-logging_2.12-3.9.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-reflect-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-api-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-log4j12-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/snappy-java-1.1.7.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/validation-api-1.1.0.Final.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zkclient-0.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zookeeper-3.4.13.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zstd-jni-1.3.8-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:29:06,948] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:29:06,949] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:29:06,949] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:29:06,949] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:29:06,949] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:29:06,949] INFO Client environment:os.version=4.4.0-18362-Microsoft (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:29:06,949] INFO Client environment:user.name=christian (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:29:06,949] INFO Client environment:user.home=/home/christian (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:29:06,950] INFO Client environment:user.dir=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:29:06,954] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@50378a4 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:29:06,980] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:29:06,980] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:29:06,989] INFO Accepted socket connection from /127.0.0.1:57927 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-08-29 14:29:06,989] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:29:07,001] INFO Client attempting to establish new session at /127.0.0.1:57927 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:29:07,013] INFO Established session 0x10000095d590001 with negotiated timeout 6000 for client /127.0.0.1:57927 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:29:07,015] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000095d590001, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:29:07,022] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:29:07,058] INFO Got user-level KeeperException when processing sessionid:0x10000095d590001 type:create cxid:0x1 zxid:0xbd2 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:29:07,076] INFO Got user-level KeeperException when processing sessionid:0x10000095d590001 type:create cxid:0x2 zxid:0xbd3 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:29:07,082] INFO Got user-level KeeperException when processing sessionid:0x10000095d590001 type:create cxid:0x3 zxid:0xbd4 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:29:07,091] INFO Got user-level KeeperException when processing sessionid:0x10000095d590001 type:create cxid:0x4 zxid:0xbd5 txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:29:07,099] INFO Got user-level KeeperException when processing sessionid:0x10000095d590001 type:create cxid:0x5 zxid:0xbd6 txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:29:07,105] INFO Got user-level KeeperException when processing sessionid:0x10000095d590001 type:create cxid:0x6 zxid:0xbd7 txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:29:07,111] INFO Got user-level KeeperException when processing sessionid:0x10000095d590001 type:create cxid:0x7 zxid:0xbd8 txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:29:07,115] INFO Got user-level KeeperException when processing sessionid:0x10000095d590001 type:create cxid:0x8 zxid:0xbd9 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:29:07,123] INFO Got user-level KeeperException when processing sessionid:0x10000095d590001 type:create cxid:0x9 zxid:0xbda txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:29:07,131] INFO Got user-level KeeperException when processing sessionid:0x10000095d590001 type:create cxid:0xa zxid:0xbdb txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:29:07,138] INFO Got user-level KeeperException when processing sessionid:0x10000095d590001 type:create cxid:0xb zxid:0xbdc txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:29:07,145] INFO Got user-level KeeperException when processing sessionid:0x10000095d590001 type:create cxid:0xc zxid:0xbdd txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:29:07,149] INFO Got user-level KeeperException when processing sessionid:0x10000095d590001 type:create cxid:0xd zxid:0xbde txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:29:07,314] INFO Cluster ID = eSotZ3p7R9WkH6FL3yanHA (kafka.server.KafkaServer)
[2019-08-29 14:29:07,317] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-08-29 14:29:07,370] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-08-29 14:29:07,397] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-08-29 14:29:07,456] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:29:07,456] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:29:07,458] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:29:07,492] INFO Log directory /tmp/kafka-logs not found, creating it. (kafka.log.LogManager)
[2019-08-29 14:29:07,507] INFO Loading logs. (kafka.log.LogManager)
[2019-08-29 14:29:07,514] INFO Logs loading complete in 7 ms. (kafka.log.LogManager)
[2019-08-29 14:29:07,541] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-08-29 14:29:07,544] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-08-29 14:29:07,924] INFO Awaiting socket connections on slocalhost:9092. (kafka.network.Acceptor)
[2019-08-29 14:29:07,961] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(localhost,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-08-29 14:29:07,963] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-08-29 14:29:07,992] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:29:07,993] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:29:07,994] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:29:07,995] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:29:08,010] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-08-29 14:29:08,059] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-08-29 14:29:08,092] INFO Stat of the created znode at /brokers/ids/0 is: 3039,3039,1567054748070,1567054748070,1,0,0,72057634258747393,188,0,3039
 (kafka.zk.KafkaZkClient)
[2019-08-29 14:29:08,094] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(localhost,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 3039 (kafka.zk.KafkaZkClient)
[2019-08-29 14:29:08,095] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-08-29 14:29:08,147] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:29:08,149] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:29:08,151] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:29:08,179] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:29:08,181] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:29:08,187] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 6 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:08,204] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:6000,blockEndProducerId:6999) by writing to Zk with path version 7 (kafka.coordinator.transaction.ProducerIdManager)
[2019-08-29 14:29:08,244] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-08-29 14:29:08,247] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-08-29 14:29:08,247] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-08-29 14:29:08,303] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-08-29 14:29:08,339] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2019-08-29 14:29:08,348] INFO Kafka version: 2.2.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-08-29 14:29:08,348] INFO Kafka commitId: 05fcfde8f69b0349 (org.apache.kafka.common.utils.AppInfoParser)
[2019-08-29 14:29:08,356] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2019-08-29 14:29:08,412] INFO Got user-level KeeperException when processing sessionid:0x10000095d590001 type:multi cxid:0x6b zxid:0xbe2 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:29:08,485] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, alerts-0, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, tdn-systrk-0, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-08-29 14:29:08,564] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,573] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 59 ms (kafka.log.Log)
[2019-08-29 14:29:08,576] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,576] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2019-08-29 14:29:08,578] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,580] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,623] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,624] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:29:08,625] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,625] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2019-08-29 14:29:08,626] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,626] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,643] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,644] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:29:08,645] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,645] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2019-08-29 14:29:08,646] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,646] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,661] INFO [Log partition=alerts-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,662] INFO [Log partition=alerts-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:29:08,665] INFO Created log for partition alerts-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,671] INFO [Partition alerts-0 broker=0] No checkpointed highwatermark is found for partition alerts-0 (kafka.cluster.Partition)
[2019-08-29 14:29:08,671] INFO Replica loaded for partition alerts-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,671] INFO [Partition alerts-0 broker=0] alerts-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,693] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,694] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:29:08,695] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,695] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2019-08-29 14:29:08,696] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,696] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,722] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,724] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-08-29 14:29:08,725] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,725] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2019-08-29 14:29:08,725] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,726] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,748] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,750] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:29:08,753] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,754] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2019-08-29 14:29:08,754] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,755] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,774] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,775] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:29:08,776] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,776] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2019-08-29 14:29:08,776] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,777] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,796] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,797] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:29:08,798] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,798] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2019-08-29 14:29:08,798] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,798] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,812] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,813] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:29:08,819] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,820] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2019-08-29 14:29:08,820] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,820] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,843] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,844] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:29:08,845] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,846] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2019-08-29 14:29:08,846] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,846] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,871] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,873] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:29:08,874] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,874] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,874] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,875] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,895] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,896] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:29:08,897] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,897] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2019-08-29 14:29:08,897] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,898] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,914] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,916] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:29:08,920] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,922] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2019-08-29 14:29:08,923] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,923] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,943] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,944] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:29:08,944] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,945] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2019-08-29 14:29:08,945] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,945] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,966] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,970] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:29:08,972] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,972] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2019-08-29 14:29:08,973] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,973] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:08,992] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:08,993] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:29:08,993] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:08,994] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2019-08-29 14:29:08,994] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:08,994] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,013] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,014] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:29:09,016] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,022] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2019-08-29 14:29:09,022] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,022] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,043] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,044] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:29:09,045] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,047] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2019-08-29 14:29:09,047] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,057] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,079] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,081] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:29:09,088] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,089] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2019-08-29 14:29:09,089] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,089] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,114] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,115] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:29:09,116] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,116] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2019-08-29 14:29:09,117] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,118] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,138] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,139] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-08-29 14:29:09,139] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,140] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2019-08-29 14:29:09,140] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,140] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,159] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,160] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:29:09,161] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,164] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2019-08-29 14:29:09,164] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,164] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,182] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,186] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-08-29 14:29:09,187] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,188] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2019-08-29 14:29:09,188] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,189] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,206] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,207] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:29:09,208] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,208] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2019-08-29 14:29:09,209] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,209] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,226] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,227] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:29:09,228] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,228] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2019-08-29 14:29:09,228] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,228] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,244] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,244] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:29:09,245] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,246] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2019-08-29 14:29:09,246] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,246] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,273] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,273] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:29:09,274] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,274] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2019-08-29 14:29:09,276] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,277] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,302] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,304] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:29:09,305] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,306] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2019-08-29 14:29:09,306] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,306] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,328] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,329] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:29:09,330] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,331] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2019-08-29 14:29:09,331] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,332] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,346] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,348] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:29:09,350] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,354] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2019-08-29 14:29:09,354] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,354] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,376] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,376] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:29:09,377] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,378] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2019-08-29 14:29:09,378] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,378] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,395] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,396] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:29:09,397] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,398] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2019-08-29 14:29:09,404] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,405] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,427] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,428] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:29:09,429] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,430] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2019-08-29 14:29:09,430] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,431] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,445] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,446] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:29:09,447] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,448] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2019-08-29 14:29:09,455] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,455] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,480] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,482] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:29:09,483] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,485] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2019-08-29 14:29:09,485] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,486] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,505] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,506] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-08-29 14:29:09,507] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,507] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2019-08-29 14:29:09,508] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,508] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,530] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,531] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:29:09,532] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,532] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2019-08-29 14:29:09,533] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,533] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,551] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,554] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-08-29 14:29:09,555] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,555] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2019-08-29 14:29:09,555] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,556] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,574] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,575] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:29:09,575] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,576] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2019-08-29 14:29:09,576] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,576] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,593] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,594] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:29:09,595] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,595] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2019-08-29 14:29:09,595] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,596] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,614] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,615] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:29:09,616] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,618] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2019-08-29 14:29:09,619] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,619] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,637] INFO [Log partition=tdn-systrk-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,641] INFO [Log partition=tdn-systrk-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:29:09,641] INFO Created log for partition tdn-systrk-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,642] INFO [Partition tdn-systrk-0 broker=0] No checkpointed highwatermark is found for partition tdn-systrk-0 (kafka.cluster.Partition)
[2019-08-29 14:29:09,642] INFO Replica loaded for partition tdn-systrk-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,642] INFO [Partition tdn-systrk-0 broker=0] tdn-systrk-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,669] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,671] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:29:09,672] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,673] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2019-08-29 14:29:09,673] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,673] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,691] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,692] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:29:09,692] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,693] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2019-08-29 14:29:09,693] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,693] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,714] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,715] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:29:09,721] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,722] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2019-08-29 14:29:09,722] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,723] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,744] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,745] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:29:09,745] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,746] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2019-08-29 14:29:09,746] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,746] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,762] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,764] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:29:09,766] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,773] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2019-08-29 14:29:09,774] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,774] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,793] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,794] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:29:09,795] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,795] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2019-08-29 14:29:09,795] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,795] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,813] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,814] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:29:09,816] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,816] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2019-08-29 14:29:09,816] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,817] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,838] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,840] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:29:09,841] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,841] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2019-08-29 14:29:09,841] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,841] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,863] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:29:09,865] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:29:09,867] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:29:09,869] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2019-08-29 14:29:09,869] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:29:09,869] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:29:09,888] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,889] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,889] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,890] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,890] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,890] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,890] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,890] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,891] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,891] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,891] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,891] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,891] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,891] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,892] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,892] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,892] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,892] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,892] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,893] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,893] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,893] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,893] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,893] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,893] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,893] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,894] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,894] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,894] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,894] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,894] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,894] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,894] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,895] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,895] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,895] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,895] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,895] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,895] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,896] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,896] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,896] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,896] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,896] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,896] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,896] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,897] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,908] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 19 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,908] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,909] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,909] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,909] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,909] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,910] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,910] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,910] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,910] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,911] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,911] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,911] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,911] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,912] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,912] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,912] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,913] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,915] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,915] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,915] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,916] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,916] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,919] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 2 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,931] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,931] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,931] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,932] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,932] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,932] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,932] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,933] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,933] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,934] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,935] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,936] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,941] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,941] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,941] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,941] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,942] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,942] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,942] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,942] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,942] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,943] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,943] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,943] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,943] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,943] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,944] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,944] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:29:09,944] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:32:06,700] INFO Unable to read additional data from server sessionid 0x10000095d590001, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:08,003] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:10,010] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:10,113] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:32:11,275] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:13,279] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:14,853] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:15,574] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:32:15,578] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2019-08-29 14:32:15,579] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)
[2019-08-29 14:32:16,855] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:16,957] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:32:18,470] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:20,474] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:21,687] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:23,691] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:25,033] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:27,037] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:28,960] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:29,666] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:32:30,884] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:32:30,965] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:31,087] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:32:31,338] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:32:31,570] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:32:31,768] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:32:31,963] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:32:33,014] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:35,018] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:35,569] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:32:35,788] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:32:37,106] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:39,109] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:40,794] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:42,799] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:43,795] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:32:44,114] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:32:44,327] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:32:44,553] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:44,601] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:32:45,701] INFO Terminating process due to signal SIGHUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:32:46,553] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:48,056] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:50,059] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:52,033] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:54,034] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:55,917] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:57,918] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:32:59,141] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:01,143] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:02,538] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:04,540] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:06,439] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:08,443] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:10,233] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:12,235] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:14,109] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:16,113] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:17,867] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:19,870] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:21,154] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:23,157] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:24,498] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:26,499] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:28,035] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:30,036] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:31,296] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:33,296] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:35,355] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:37,357] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:33:37,548] ERROR Error while writing to checkpoint file /tmp/kafka-logs/log-start-offset-checkpoint (kafka.server.LogDirFailureChannel)
java.io.FileNotFoundException: /tmp/kafka-logs/log-start-offset-checkpoint.tmp (No such file or directory)
	at java.base/java.io.FileOutputStream.open0(Native Method)
	at java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)
	at java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)
	at java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:187)
	at kafka.server.checkpoints.CheckpointFile.liftedTree1$1(CheckpointFile.scala:52)
	at kafka.server.checkpoints.CheckpointFile.write(CheckpointFile.scala:50)
	at kafka.server.checkpoints.OffsetCheckpointFile.write(OffsetCheckpointFile.scala:59)
	at kafka.log.LogManager.$anonfun$checkpointLogStartOffsetsInDir$2(LogManager.scala:626)
	at kafka.log.LogManager.$anonfun$checkpointLogStartOffsetsInDir$2$adapted(LogManager.scala:620)
	at scala.Option.foreach(Option.scala:274)
	at kafka.log.LogManager.$anonfun$checkpointLogStartOffsetsInDir$1(LogManager.scala:620)
	at kafka.log.LogManager.$anonfun$checkpointLogStartOffsetsInDir$1$adapted(LogManager.scala:619)
	at scala.Option.foreach(Option.scala:274)
	at kafka.log.LogManager.checkpointLogStartOffsetsInDir(LogManager.scala:619)
	at kafka.log.LogManager.$anonfun$checkpointLogStartOffsets$1(LogManager.scala:584)
	at kafka.log.LogManager.$anonfun$checkpointLogStartOffsets$1$adapted(LogManager.scala:584)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at kafka.log.LogManager.checkpointLogStartOffsets(LogManager.scala:584)
	at kafka.log.LogManager.$anonfun$startup$6(LogManager.scala:411)
	at kafka.utils.KafkaScheduler.$anonfun$schedule$2(KafkaScheduler.scala:114)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:33:37,548] ERROR Error while writing to checkpoint file /tmp/kafka-logs/recovery-point-offset-checkpoint (kafka.server.LogDirFailureChannel)
java.io.FileNotFoundException: /tmp/kafka-logs/recovery-point-offset-checkpoint.tmp (No such file or directory)
	at java.base/java.io.FileOutputStream.open0(Native Method)
	at java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)
	at java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)
	at java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:187)
	at kafka.server.checkpoints.CheckpointFile.liftedTree1$1(CheckpointFile.scala:52)
	at kafka.server.checkpoints.CheckpointFile.write(CheckpointFile.scala:50)
	at kafka.server.checkpoints.OffsetCheckpointFile.write(OffsetCheckpointFile.scala:59)
	at kafka.log.LogManager.$anonfun$checkpointLogRecoveryOffsetsInDir$2(LogManager.scala:610)
	at kafka.log.LogManager.$anonfun$checkpointLogRecoveryOffsetsInDir$2$adapted(LogManager.scala:608)
	at scala.Option.foreach(Option.scala:274)
	at kafka.log.LogManager.$anonfun$checkpointLogRecoveryOffsetsInDir$1(LogManager.scala:608)
	at kafka.log.LogManager.$anonfun$checkpointLogRecoveryOffsetsInDir$1$adapted(LogManager.scala:607)
	at scala.Option.foreach(Option.scala:274)
	at kafka.log.LogManager.checkpointLogRecoveryOffsetsInDir(LogManager.scala:607)
	at kafka.log.LogManager.checkpointRecoveryOffsetsAndCleanSnapshot(LogManager.scala:596)
	at kafka.log.LogManager.$anonfun$checkpointLogRecoveryOffsets$3(LogManager.scala:574)
	at kafka.log.LogManager.$anonfun$checkpointLogRecoveryOffsets$3$adapted(LogManager.scala:573)
	at scala.Option.foreach(Option.scala:274)
	at kafka.log.LogManager.$anonfun$checkpointLogRecoveryOffsets$1(LogManager.scala:573)
	at kafka.log.LogManager.$anonfun$checkpointLogRecoveryOffsets$1$adapted(LogManager.scala:572)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:128)
	at kafka.log.LogManager.checkpointLogRecoveryOffsets(LogManager.scala:572)
	at kafka.log.LogManager.$anonfun$startup$5(LogManager.scala:406)
	at kafka.utils.KafkaScheduler.$anonfun$schedule$2(KafkaScheduler.scala:114)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:33:37,552] ERROR Uncaught exception in scheduled task 'kafka-log-start-offset-checkpoint' (kafka.utils.KafkaScheduler)
org.apache.kafka.common.errors.KafkaStorageException: Error while writing to checkpoint file /tmp/kafka-logs/log-start-offset-checkpoint
Caused by: java.io.FileNotFoundException: /tmp/kafka-logs/log-start-offset-checkpoint.tmp (No such file or directory)
	at java.base/java.io.FileOutputStream.open0(Native Method)
	at java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)
	at java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)
	at java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:187)
	at kafka.server.checkpoints.CheckpointFile.liftedTree1$1(CheckpointFile.scala:52)
	at kafka.server.checkpoints.CheckpointFile.write(CheckpointFile.scala:50)
	at kafka.server.checkpoints.OffsetCheckpointFile.write(OffsetCheckpointFile.scala:59)
	at kafka.log.LogManager.$anonfun$checkpointLogStartOffsetsInDir$2(LogManager.scala:626)
	at kafka.log.LogManager.$anonfun$checkpointLogStartOffsetsInDir$2$adapted(LogManager.scala:620)
	at scala.Option.foreach(Option.scala:274)
	at kafka.log.LogManager.$anonfun$checkpointLogStartOffsetsInDir$1(LogManager.scala:620)
	at kafka.log.LogManager.$anonfun$checkpointLogStartOffsetsInDir$1$adapted(LogManager.scala:619)
	at scala.Option.foreach(Option.scala:274)
	at kafka.log.LogManager.checkpointLogStartOffsetsInDir(LogManager.scala:619)
	at kafka.log.LogManager.$anonfun$checkpointLogStartOffsets$1(LogManager.scala:584)
	at kafka.log.LogManager.$anonfun$checkpointLogStartOffsets$1$adapted(LogManager.scala:584)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at kafka.log.LogManager.checkpointLogStartOffsets(LogManager.scala:584)
	at kafka.log.LogManager.$anonfun$startup$6(LogManager.scala:411)
	at kafka.utils.KafkaScheduler.$anonfun$schedule$2(KafkaScheduler.scala:114)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:33:37,552] INFO [ReplicaManager broker=0] Stopping serving replicas in dir /tmp/kafka-logs (kafka.server.ReplicaManager)
[2019-08-29 14:33:37,553] ERROR Uncaught exception in scheduled task 'kafka-recovery-point-checkpoint' (kafka.utils.KafkaScheduler)
org.apache.kafka.common.errors.KafkaStorageException: Error while writing to checkpoint file /tmp/kafka-logs/recovery-point-offset-checkpoint
Caused by: java.io.FileNotFoundException: /tmp/kafka-logs/recovery-point-offset-checkpoint.tmp (No such file or directory)
	at java.base/java.io.FileOutputStream.open0(Native Method)
	at java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)
	at java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)
	at java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:187)
	at kafka.server.checkpoints.CheckpointFile.liftedTree1$1(CheckpointFile.scala:52)
	at kafka.server.checkpoints.CheckpointFile.write(CheckpointFile.scala:50)
	at kafka.server.checkpoints.OffsetCheckpointFile.write(OffsetCheckpointFile.scala:59)
	at kafka.log.LogManager.$anonfun$checkpointLogRecoveryOffsetsInDir$2(LogManager.scala:610)
	at kafka.log.LogManager.$anonfun$checkpointLogRecoveryOffsetsInDir$2$adapted(LogManager.scala:608)
	at scala.Option.foreach(Option.scala:274)
	at kafka.log.LogManager.$anonfun$checkpointLogRecoveryOffsetsInDir$1(LogManager.scala:608)
	at kafka.log.LogManager.$anonfun$checkpointLogRecoveryOffsetsInDir$1$adapted(LogManager.scala:607)
	at scala.Option.foreach(Option.scala:274)
	at kafka.log.LogManager.checkpointLogRecoveryOffsetsInDir(LogManager.scala:607)
	at kafka.log.LogManager.checkpointRecoveryOffsetsAndCleanSnapshot(LogManager.scala:596)
	at kafka.log.LogManager.$anonfun$checkpointLogRecoveryOffsets$3(LogManager.scala:574)
	at kafka.log.LogManager.$anonfun$checkpointLogRecoveryOffsets$3$adapted(LogManager.scala:573)
	at scala.Option.foreach(Option.scala:274)
	at kafka.log.LogManager.$anonfun$checkpointLogRecoveryOffsets$1(LogManager.scala:573)
	at kafka.log.LogManager.$anonfun$checkpointLogRecoveryOffsets$1$adapted(LogManager.scala:572)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:128)
	at kafka.log.LogManager.checkpointLogRecoveryOffsets(LogManager.scala:572)
	at kafka.log.LogManager.$anonfun$startup$5(LogManager.scala:406)
	at kafka.utils.KafkaScheduler.$anonfun$schedule$2(KafkaScheduler.scala:114)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:33:37,556] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, alerts-0, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, tdn-systrk-0, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-08-29 14:33:37,557] INFO [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, alerts-0, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, tdn-systrk-0, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2019-08-29 14:33:37,576] INFO [ReplicaManager broker=0] Broker 0 stopped fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,alerts-0,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,tdn-systrk-0,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 and stopped moving logs for partitions  because they are in the failed log directory /tmp/kafka-logs. (kafka.server.ReplicaManager)
[2019-08-29 14:33:37,576] INFO Stopping serving logs in dir /tmp/kafka-logs (kafka.log.LogManager)
[2019-08-29 14:33:37,579] ERROR Shutdown broker because all log dirs in /tmp/kafka-logs have failed (kafka.log.LogManager)
[2019-08-29 14:33:53,558] INFO Reading configuration from: ./kafka_2.12-2.2.0/config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-08-29 14:33:53,568] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-08-29 14:33:53,569] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-08-29 14:33:53,570] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-08-29 14:33:53,576] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-08-29 14:33:53,602] INFO Reading configuration from: ./kafka_2.12-2.2.0/config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-08-29 14:33:53,604] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-08-29 14:33:53,611] INFO Server environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,611] INFO Server environment:host.name=Christian-Inspiron.localdomain (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,616] INFO Server environment:java.version=11.0.4 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,617] INFO Server environment:java.vendor=Ubuntu (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,623] INFO Server environment:java.home=/usr/lib/jvm/java-11-openjdk-amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,623] INFO Server environment:java.class.path=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/activation-1.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/aopalliance-repackaged-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/argparse4j-0.7.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/audience-annotations-0.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/commons-lang3-3.8.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-api-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-basic-auth-extension-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-file-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-json-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-runtime-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-transforms-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/guava-20.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-api-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-locator-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-utils-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-core-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-databind-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-datatype-jdk8-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-base-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-json-provider-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-module-jaxb-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javassist-3.22.0-CR2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.annotation-api-1.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.servlet-api-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jaxb-api-2.3.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-client-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-common-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-core-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-hk2-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-media-jaxb-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-server-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-client-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-continuation-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-http-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-io-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-security-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-server-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlet-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlets-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-util-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jopt-simple-5.0.4.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-clients-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-log4j-appender-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-examples-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-scala_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-test-utils-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-tools-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0-sources.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/log4j-1.2.17.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/lz4-java-1.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/maven-artifact-3.6.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/metrics-core-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/osgi-resource-locator-1.0.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/plexus-utils-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/reflections-0.9.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/rocksdbjni-5.15.10.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-library-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-logging_2.12-3.9.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-reflect-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-api-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-log4j12-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/snappy-java-1.1.7.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/validation-api-1.1.0.Final.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zkclient-0.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zookeeper-3.4.13.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zstd-jni-1.3.8-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,628] INFO Server environment:java.library.path=/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,628] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,628] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,628] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,628] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,629] INFO Server environment:os.version=4.4.0-18362-Microsoft (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,629] INFO Server environment:user.name=christian (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,629] INFO Server environment:user.home=/home/christian (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,629] INFO Server environment:user.dir=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,642] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,645] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,645] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:33:53,655] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-08-29 14:33:53,662] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-08-29 14:34:00,357] INFO Expiring session 0x10000095d590001, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:34:00,363] INFO Processed session termination for sessionid: 0x10000095d590001 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:34:00,365] INFO Creating new log file: log.c03 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-08-29 14:34:25,008] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-08-29 14:34:25,367] INFO starting (kafka.server.KafkaServer)
[2019-08-29 14:34:25,368] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-08-29 14:34:25,384] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:34:25,388] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:34:25,388] INFO Client environment:host.name=Christian-Inspiron.localdomain (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:34:25,388] INFO Client environment:java.version=11.0.4 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:34:25,388] INFO Client environment:java.vendor=Ubuntu (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:34:25,388] INFO Client environment:java.home=/usr/lib/jvm/java-11-openjdk-amd64 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:34:25,388] INFO Client environment:java.class.path=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/activation-1.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/aopalliance-repackaged-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/argparse4j-0.7.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/audience-annotations-0.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/commons-lang3-3.8.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-api-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-basic-auth-extension-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-file-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-json-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-runtime-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-transforms-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/guava-20.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-api-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-locator-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-utils-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-core-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-databind-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-datatype-jdk8-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-base-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-json-provider-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-module-jaxb-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javassist-3.22.0-CR2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.annotation-api-1.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.servlet-api-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jaxb-api-2.3.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-client-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-common-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-core-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-hk2-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-media-jaxb-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-server-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-client-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-continuation-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-http-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-io-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-security-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-server-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlet-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlets-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-util-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jopt-simple-5.0.4.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-clients-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-log4j-appender-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-examples-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-scala_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-test-utils-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-tools-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0-sources.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/log4j-1.2.17.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/lz4-java-1.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/maven-artifact-3.6.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/metrics-core-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/osgi-resource-locator-1.0.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/plexus-utils-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/reflections-0.9.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/rocksdbjni-5.15.10.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-library-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-logging_2.12-3.9.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-reflect-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-api-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-log4j12-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/snappy-java-1.1.7.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/validation-api-1.1.0.Final.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zkclient-0.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zookeeper-3.4.13.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zstd-jni-1.3.8-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:34:25,390] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:34:25,391] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:34:25,391] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:34:25,392] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:34:25,392] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:34:25,393] INFO Client environment:os.version=4.4.0-18362-Microsoft (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:34:25,402] INFO Client environment:user.name=christian (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:34:25,405] INFO Client environment:user.home=/home/christian (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:34:25,406] INFO Client environment:user.dir=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:34:25,407] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@50378a4 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:34:25,423] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:34:25,423] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:34:25,430] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:34:25,431] INFO Accepted socket connection from /127.0.0.1:58251 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-08-29 14:34:25,438] INFO Client attempting to establish new session at /127.0.0.1:58251 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:34:25,456] INFO Established session 0x100000fd48e0000 with negotiated timeout 6000 for client /127.0.0.1:58251 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:34:25,458] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100000fd48e0000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:34:25,463] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:34:25,503] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0000 type:create cxid:0x1 zxid:0xc05 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:34:25,515] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0000 type:create cxid:0x2 zxid:0xc06 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:34:25,522] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0000 type:create cxid:0x3 zxid:0xc07 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:34:25,528] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0000 type:create cxid:0x4 zxid:0xc08 txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:34:25,531] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0000 type:create cxid:0x5 zxid:0xc09 txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:34:25,534] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0000 type:create cxid:0x6 zxid:0xc0a txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:34:25,541] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0000 type:create cxid:0x7 zxid:0xc0b txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:34:25,545] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0000 type:create cxid:0x8 zxid:0xc0c txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:34:25,547] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0000 type:create cxid:0x9 zxid:0xc0d txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:34:25,550] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0000 type:create cxid:0xa zxid:0xc0e txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:34:25,556] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0000 type:create cxid:0xb zxid:0xc0f txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:34:25,562] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0000 type:create cxid:0xc zxid:0xc10 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:34:25,565] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0000 type:create cxid:0xd zxid:0xc11 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:34:25,704] INFO Cluster ID = eSotZ3p7R9WkH6FL3yanHA (kafka.server.KafkaServer)
[2019-08-29 14:34:25,707] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-08-29 14:34:25,751] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-08-29 14:34:26,031] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-08-29 14:34:26,113] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:34:26,113] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:34:26,114] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:34:26,142] INFO Log directory /tmp/kafka-logs not found, creating it. (kafka.log.LogManager)
[2019-08-29 14:34:26,150] INFO Loading logs. (kafka.log.LogManager)
[2019-08-29 14:34:26,161] INFO Logs loading complete in 11 ms. (kafka.log.LogManager)
[2019-08-29 14:34:26,175] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-08-29 14:34:26,179] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-08-29 14:34:26,497] INFO Awaiting socket connections on slocalhost:9092. (kafka.network.Acceptor)
[2019-08-29 14:34:26,526] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(localhost,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-08-29 14:34:26,529] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-08-29 14:34:26,550] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:34:26,551] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:34:26,553] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:34:26,556] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:34:26,569] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-08-29 14:34:26,613] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-08-29 14:34:26,635] INFO Stat of the created znode at /brokers/ids/0 is: 3090,3090,1567055066622,1567055066622,1,0,0,72057662028513280,188,0,3090
 (kafka.zk.KafkaZkClient)
[2019-08-29 14:34:26,637] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(localhost,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 3090 (kafka.zk.KafkaZkClient)
[2019-08-29 14:34:26,639] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-08-29 14:34:26,686] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:34:26,690] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:34:26,691] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:34:26,716] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:34:26,718] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:34:26,722] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 4 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:26,741] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:7000,blockEndProducerId:7999) by writing to Zk with path version 8 (kafka.coordinator.transaction.ProducerIdManager)
[2019-08-29 14:34:26,775] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-08-29 14:34:26,780] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-08-29 14:34:26,780] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-08-29 14:34:26,822] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-08-29 14:34:26,846] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2019-08-29 14:34:26,854] INFO Kafka version: 2.2.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-08-29 14:34:26,854] INFO Kafka commitId: 05fcfde8f69b0349 (org.apache.kafka.common.utils.AppInfoParser)
[2019-08-29 14:34:26,855] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2019-08-29 14:34:26,889] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0000 type:multi cxid:0x6b zxid:0xc15 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:34:26,943] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, alerts-0, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, tdn-systrk-0, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-08-29 14:34:26,995] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,002] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 44 ms (kafka.log.Log)
[2019-08-29 14:34:27,003] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,004] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2019-08-29 14:34:27,008] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,011] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,040] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,041] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:34:27,042] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,042] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2019-08-29 14:34:27,043] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,044] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,059] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,061] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:34:27,063] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,063] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2019-08-29 14:34:27,063] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,063] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,083] INFO [Log partition=alerts-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,084] INFO [Log partition=alerts-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:34:27,089] INFO Created log for partition alerts-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,091] INFO [Partition alerts-0 broker=0] No checkpointed highwatermark is found for partition alerts-0 (kafka.cluster.Partition)
[2019-08-29 14:34:27,091] INFO Replica loaded for partition alerts-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,092] INFO [Partition alerts-0 broker=0] alerts-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,109] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,111] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:34:27,112] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,112] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2019-08-29 14:34:27,112] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,113] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,125] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,127] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:34:27,128] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,128] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2019-08-29 14:34:27,128] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,128] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,146] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,147] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:34:27,148] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,148] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2019-08-29 14:34:27,148] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,149] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,166] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,167] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:34:27,167] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,168] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2019-08-29 14:34:27,168] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,168] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,183] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,183] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:34:27,184] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,184] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2019-08-29 14:34:27,184] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,185] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,198] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,199] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:34:27,199] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,200] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2019-08-29 14:34:27,201] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,202] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,216] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,216] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:34:27,217] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,217] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2019-08-29 14:34:27,217] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,218] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,234] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,235] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:34:27,236] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,236] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,236] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,237] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,250] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,251] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:34:27,252] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,252] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2019-08-29 14:34:27,253] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,253] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,281] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,283] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:34:27,283] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,284] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2019-08-29 14:34:27,284] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,284] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,311] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,312] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-08-29 14:34:27,313] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,313] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2019-08-29 14:34:27,313] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,314] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,331] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,332] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:34:27,332] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,333] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2019-08-29 14:34:27,333] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,333] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,348] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,349] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:34:27,349] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,350] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2019-08-29 14:34:27,350] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,350] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,365] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,366] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:34:27,367] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,368] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2019-08-29 14:34:27,368] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,369] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,384] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,385] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:34:27,385] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,386] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2019-08-29 14:34:27,386] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,387] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,404] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,405] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:34:27,405] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,406] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2019-08-29 14:34:27,406] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,406] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,423] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,424] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:34:27,424] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,425] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2019-08-29 14:34:27,425] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,425] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,445] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,446] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:34:27,446] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,447] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2019-08-29 14:34:27,447] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,447] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,463] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,464] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:34:27,465] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,465] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2019-08-29 14:34:27,465] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,466] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,488] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,489] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:34:27,490] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,490] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2019-08-29 14:34:27,490] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,491] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,513] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,515] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:34:27,515] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,516] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2019-08-29 14:34:27,516] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,516] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,531] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,532] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:34:27,533] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,533] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2019-08-29 14:34:27,533] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,533] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,548] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,548] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:34:27,549] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,549] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2019-08-29 14:34:27,550] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,550] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,566] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,567] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:34:27,568] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,568] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2019-08-29 14:34:27,568] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,569] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,582] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,583] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:34:27,583] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,584] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2019-08-29 14:34:27,584] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,584] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,605] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,607] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:34:27,608] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,608] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2019-08-29 14:34:27,608] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,609] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,624] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,625] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:34:27,627] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,628] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2019-08-29 14:34:27,628] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,629] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,650] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,650] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:34:27,654] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,655] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2019-08-29 14:34:27,656] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,657] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,670] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,671] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:34:27,673] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,674] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2019-08-29 14:34:27,674] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,675] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,698] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,700] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:34:27,700] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,701] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2019-08-29 14:34:27,701] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,701] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,721] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,723] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:34:27,724] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,724] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2019-08-29 14:34:27,725] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,725] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,741] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,741] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:34:27,745] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,745] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2019-08-29 14:34:27,745] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,745] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,768] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,769] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:34:27,773] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,774] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2019-08-29 14:34:27,774] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,774] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,787] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,788] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:34:27,790] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,791] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2019-08-29 14:34:27,796] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,796] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,817] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,818] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:34:27,819] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,820] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2019-08-29 14:34:27,820] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,820] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,837] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,837] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:34:27,838] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,840] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2019-08-29 14:34:27,840] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,840] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,863] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,864] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:34:27,865] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,865] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2019-08-29 14:34:27,866] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,866] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,881] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,882] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:34:27,882] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,883] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2019-08-29 14:34:27,885] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,885] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,899] INFO [Log partition=tdn-systrk-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,899] INFO [Log partition=tdn-systrk-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:34:27,900] INFO Created log for partition tdn-systrk-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,900] INFO [Partition tdn-systrk-0 broker=0] No checkpointed highwatermark is found for partition tdn-systrk-0 (kafka.cluster.Partition)
[2019-08-29 14:34:27,900] INFO Replica loaded for partition tdn-systrk-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,901] INFO [Partition tdn-systrk-0 broker=0] tdn-systrk-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,917] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,918] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:34:27,920] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,920] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2019-08-29 14:34:27,921] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,921] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,934] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,935] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:34:27,936] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,936] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2019-08-29 14:34:27,937] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,941] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,955] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,956] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:34:27,958] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,960] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2019-08-29 14:34:27,963] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,963] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,980] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,980] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:34:27,981] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:27,981] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2019-08-29 14:34:27,981] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:27,982] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:27,998] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:27,998] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:34:27,999] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:28,000] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2019-08-29 14:34:28,000] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:28,000] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:28,015] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:28,015] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:34:28,016] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:28,016] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2019-08-29 14:34:28,016] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:28,016] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:28,032] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:28,033] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:34:28,033] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:28,033] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2019-08-29 14:34:28,036] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:28,037] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:28,051] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:28,052] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:34:28,053] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:28,054] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2019-08-29 14:34:28,054] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:28,055] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:28,073] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:34:28,074] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:34:28,074] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:34:28,075] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2019-08-29 14:34:28,075] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:34:28,075] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:34:28,092] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,094] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,095] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,095] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,095] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,095] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,095] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,095] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,095] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,096] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,096] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,096] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,096] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,096] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,097] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,097] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,097] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,097] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,097] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,098] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,098] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,098] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,098] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,098] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,098] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,098] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,099] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,099] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,099] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,099] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,099] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,099] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,099] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,100] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,100] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,100] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,100] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,100] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,100] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,100] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,100] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,101] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,101] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,101] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,101] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,101] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,101] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,101] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,101] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,101] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,106] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 12 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,113] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,114] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,114] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,114] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,114] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,115] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,115] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,115] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,115] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,116] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,116] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,116] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,116] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,117] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,117] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,117] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,117] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,117] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,118] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,118] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,118] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,118] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,119] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,120] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,120] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,120] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,121] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,121] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,121] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,121] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,121] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,122] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,128] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,129] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,129] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,129] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,129] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,129] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,130] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,130] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,130] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,130] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,130] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,131] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,131] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,131] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,131] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,131] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:34:28,132] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:43:21,474] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:43:21,476] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2019-08-29 14:43:21,477] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)
[2019-08-29 14:43:21,502] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2019-08-29 14:43:21,510] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-08-29 14:43:21,511] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-08-29 14:43:21,511] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-08-29 14:43:21,512] INFO [SocketServer brokerId=0] Stopping socket server request processors (kafka.network.SocketServer)
[2019-08-29 14:43:21,538] INFO [SocketServer brokerId=0] Stopped socket server request processors (kafka.network.SocketServer)
[2019-08-29 14:43:21,539] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool)
[2019-08-29 14:43:21,543] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2019-08-29 14:43:21,547] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis)
[2019-08-29 14:43:21,551] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:21,576] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:21,576] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:21,581] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-08-29 14:43:21,582] INFO [ProducerId Manager 0]: Shutdown complete: last producerId assigned 7000 (kafka.coordinator.transaction.ProducerIdManager)
[2019-08-29 14:43:21,583] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2019-08-29 14:43:21,583] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-08-29 14:43:21,583] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-08-29 14:43:21,583] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-08-29 14:43:21,584] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-08-29 14:43:21,584] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:43:21,585] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:21,777] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:21,777] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:21,778] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:21,783] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:21,783] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:21,784] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:43:21,787] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)
[2019-08-29 14:43:21,792] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-08-29 14:43:21,793] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-08-29 14:43:21,793] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-08-29 14:43:21,794] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)
[2019-08-29 14:43:21,798] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)
[2019-08-29 14:43:21,799] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2019-08-29 14:43:21,802] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2019-08-29 14:43:21,803] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:21,841] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:21,841] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:21,844] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:22,037] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:22,037] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:22,039] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:22,238] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:22,238] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:22,239] INFO [ExpirationReaper-0-ElectPreferredLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:22,438] INFO [ExpirationReaper-0-ElectPreferredLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:22,438] INFO [ExpirationReaper-0-ElectPreferredLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:43:22,457] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)
[2019-08-29 14:43:22,457] INFO Shutting down. (kafka.log.LogManager)
[2019-08-29 14:43:22,465] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,479] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,485] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,495] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,500] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,510] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,515] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,518] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,534] INFO [ProducerStateManager partition=alerts-0] Writing producer snapshot at offset 1 (kafka.log.ProducerStateManager)
[2019-08-29 14:43:22,543] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,546] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,558] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,561] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,572] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,576] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,587] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,590] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,603] INFO [ProducerStateManager partition=tdn-systrk-0] Writing producer snapshot at offset 4532 (kafka.log.ProducerStateManager)
[2019-08-29 14:43:22,604] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,608] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,627] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,635] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,643] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,651] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,657] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,666] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,672] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,683] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,688] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,698] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,703] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,705] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,717] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,720] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,732] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,735] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,739] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,748] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,753] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,763] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,768] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,778] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,784] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,800] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,805] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,815] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,819] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,821] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,832] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,835] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,846] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,849] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,853] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,862] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,868] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,883] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,889] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,899] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,902] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,905] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,917] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,920] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,932] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,934] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,938] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,947] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,951] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,961] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,966] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,968] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,983] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,985] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:22,996] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,000] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,013] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,016] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,028] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,030] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,036] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,051] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,058] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,067] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,071] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,082] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,087] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,103] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,109] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,118] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,124] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,140] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,146] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,153] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,158] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,167] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,171] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,175] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,186] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,188] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,201] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,203] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,209] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,218] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,222] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,235] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,244] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:563)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:563)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,252] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:247)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:564)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:564)
	at kafka.log.Log.$anonfun$close$4(Log.scala:747)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:747)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.log.Log.$anonfun$close$3(Log.scala:747)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2013)
	at kafka.log.Log.close(Log.scala:742)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:458)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:43:23,278] INFO Shutdown complete. (kafka.log.LogManager)
[2019-08-29 14:43:23,285] INFO [ZooKeeperClient] Closing. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:43:23,286] INFO Processed session termination for sessionid: 0x100000fd48e0000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:43:23,292] INFO Session: 0x100000fd48e0000 closed (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:43:23,296] INFO Closed socket connection for client /127.0.0.1:58251 which had sessionid 0x100000fd48e0000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-08-29 14:43:23,296] INFO EventThread shut down for session: 0x100000fd48e0000 (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:43:23,296] INFO [ZooKeeperClient] Closed. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:43:23,302] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:43:23,321] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:43:23,321] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:43:23,321] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:43:24,322] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:43:24,322] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:43:24,324] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:43:25,323] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:43:25,323] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:43:25,328] INFO [SocketServer brokerId=0] Shutting down socket server (kafka.network.SocketServer)
[2019-08-29 14:43:25,345] INFO [SocketServer brokerId=0] Shutdown completed (kafka.network.SocketServer)
[2019-08-29 14:43:25,348] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)
[2019-08-29 14:45:07,119] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-08-29 14:45:07,482] INFO starting (kafka.server.KafkaServer)
[2019-08-29 14:45:07,483] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-08-29 14:45:07,498] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:45:07,503] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:07,503] INFO Client environment:host.name=Christian-Inspiron.localdomain (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:07,503] INFO Client environment:java.version=11.0.4 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:07,503] INFO Client environment:java.vendor=Ubuntu (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:07,503] INFO Client environment:java.home=/usr/lib/jvm/java-11-openjdk-amd64 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:07,503] INFO Client environment:java.class.path=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/activation-1.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/aopalliance-repackaged-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/argparse4j-0.7.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/audience-annotations-0.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/commons-lang3-3.8.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-api-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-basic-auth-extension-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-file-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-json-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-runtime-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-transforms-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/guava-20.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-api-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-locator-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-utils-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-core-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-databind-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-datatype-jdk8-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-base-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-json-provider-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-module-jaxb-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javassist-3.22.0-CR2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.annotation-api-1.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.servlet-api-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jaxb-api-2.3.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-client-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-common-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-core-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-hk2-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-media-jaxb-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-server-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-client-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-continuation-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-http-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-io-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-security-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-server-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlet-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlets-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-util-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jopt-simple-5.0.4.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-clients-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-log4j-appender-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-examples-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-scala_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-test-utils-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-tools-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0-sources.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/log4j-1.2.17.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/lz4-java-1.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/maven-artifact-3.6.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/metrics-core-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/osgi-resource-locator-1.0.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/plexus-utils-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/reflections-0.9.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/rocksdbjni-5.15.10.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-library-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-logging_2.12-3.9.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-reflect-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-api-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-log4j12-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/snappy-java-1.1.7.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/validation-api-1.1.0.Final.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zkclient-0.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zookeeper-3.4.13.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zstd-jni-1.3.8-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:07,506] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:07,506] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:07,506] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:07,506] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:07,506] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:07,506] INFO Client environment:os.version=4.4.0-18362-Microsoft (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:07,506] INFO Client environment:user.name=christian (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:07,506] INFO Client environment:user.home=/home/christian (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:07,507] INFO Client environment:user.dir=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:07,508] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@50378a4 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:07,521] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:45:07,521] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:45:07,528] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:45:07,528] INFO Accepted socket connection from /127.0.0.1:58749 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-08-29 14:45:07,531] INFO Client attempting to establish new session at /127.0.0.1:58749 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:45:07,542] INFO Established session 0x100000fd48e0001 with negotiated timeout 6000 for client /127.0.0.1:58749 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:45:07,545] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100000fd48e0001, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:45:07,552] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:45:07,581] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0001 type:create cxid:0x1 zxid:0xc7a txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:07,591] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0001 type:create cxid:0x2 zxid:0xc7b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:07,594] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0001 type:create cxid:0x3 zxid:0xc7c txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:07,596] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0001 type:create cxid:0x4 zxid:0xc7d txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:07,599] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0001 type:create cxid:0x5 zxid:0xc7e txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:07,601] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0001 type:create cxid:0x6 zxid:0xc7f txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:07,604] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0001 type:create cxid:0x7 zxid:0xc80 txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:07,607] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0001 type:create cxid:0x8 zxid:0xc81 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:07,612] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0001 type:create cxid:0x9 zxid:0xc82 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:07,618] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0001 type:create cxid:0xa zxid:0xc83 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:07,627] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0001 type:create cxid:0xb zxid:0xc84 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:07,632] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0001 type:create cxid:0xc zxid:0xc85 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:07,637] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0001 type:create cxid:0xd zxid:0xc86 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:07,768] INFO Cluster ID = eSotZ3p7R9WkH6FL3yanHA (kafka.server.KafkaServer)
[2019-08-29 14:45:07,810] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-08-29 14:45:07,831] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-08-29 14:45:07,869] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:45:07,869] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:45:07,871] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:45:07,901] INFO Loading logs. (kafka.log.LogManager)
[2019-08-29 14:45:07,959] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:07,986] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 64 ms (kafka.log.Log)
[2019-08-29 14:45:08,013] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,015] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[2019-08-29 14:45:08,033] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,035] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:45:08,056] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,058] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[2019-08-29 14:45:08,080] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,086] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 22 ms (kafka.log.Log)
[2019-08-29 14:45:08,106] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,107] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-08-29 14:45:08,119] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,119] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:45:08,129] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,129] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:45:08,146] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,148] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-08-29 14:45:08,168] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,171] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[2019-08-29 14:45:08,185] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,186] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:45:08,195] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,195] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:45:08,212] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,213] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[2019-08-29 14:45:08,227] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,227] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-08-29 14:45:08,241] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,243] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:45:08,257] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,257] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:45:08,270] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,270] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:45:08,282] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,283] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:45:08,295] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,296] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:45:08,307] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,307] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:45:08,330] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,331] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 22 ms (kafka.log.Log)
[2019-08-29 14:45:08,345] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,345] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-08-29 14:45:08,356] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,356] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-08-29 14:45:08,369] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,371] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-08-29 14:45:08,390] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,392] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[2019-08-29 14:45:08,408] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,408] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-08-29 14:45:08,424] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,426] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[2019-08-29 14:45:08,440] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,440] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:45:08,451] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,452] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:45:08,466] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,468] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-08-29 14:45:08,486] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,488] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-08-29 14:45:08,506] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,508] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[2019-08-29 14:45:08,524] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,524] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-08-29 14:45:08,540] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,542] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[2019-08-29 14:45:08,555] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,555] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:45:08,568] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,569] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-08-29 14:45:08,581] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,583] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-08-29 14:45:08,593] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,594] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:45:08,605] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,605] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:45:08,615] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,616] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:45:08,629] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,630] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:45:08,639] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,640] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-08-29 14:45:08,652] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,653] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-08-29 14:45:08,663] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,663] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-08-29 14:45:08,676] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,677] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-08-29 14:45:08,685] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,685] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:45:08,696] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,696] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-08-29 14:45:08,712] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,714] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-08-29 14:45:08,730] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,730] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-08-29 14:45:08,740] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,741] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:45:08,745] WARN [Log partition=alerts-0, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/alerts-0/00000000000000000000.log due to Corrupt time index found, time index file (/tmp/kafka-logs/alerts-0/00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1567055550142}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-08-29 14:45:08,746] INFO [Log partition=alerts-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,767] ERROR Error while loading log dir /tmp/kafka-logs (kafka.log.LogManager)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.LogSegment.recover(LogSegment.scala:377)
	at kafka.log.Log.recoverSegment(Log.scala:500)
	at kafka.log.Log.$anonfun$loadSegmentFiles$3(Log.scala:482)
	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:792)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:791)
	at kafka.log.Log.loadSegmentFiles(Log.scala:454)
	at kafka.log.Log.$anonfun$loadSegments$1(Log.scala:565)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2024)
	at kafka.log.Log.loadSegments(Log.scala:559)
	at kafka.log.Log.<init>(Log.scala:292)
	at kafka.log.Log$.apply(Log.scala:2158)
	at kafka.log.LogManager.loadLog(LogManager.scala:275)
	at kafka.log.LogManager.$anonfun$loadLogs$12(LogManager.scala:345)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:45:08,774] WARN [Log partition=tdn-systrk-0, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/tdn-systrk-0/00000000000000000000.log due to Corrupt time index found, time index file (/tmp/kafka-logs/tdn-systrk-0/00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1567055159108}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-08-29 14:45:08,774] INFO [Log partition=tdn-systrk-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:08,821] ERROR Error while loading log dir /tmp/kafka-logs (kafka.log.LogManager)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.LogSegment.recover(LogSegment.scala:377)
	at kafka.log.Log.recoverSegment(Log.scala:500)
	at kafka.log.Log.$anonfun$loadSegmentFiles$3(Log.scala:482)
	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:792)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:791)
	at kafka.log.Log.loadSegmentFiles(Log.scala:454)
	at kafka.log.Log.$anonfun$loadSegments$1(Log.scala:565)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2024)
	at kafka.log.Log.loadSegments(Log.scala:559)
	at kafka.log.Log.<init>(Log.scala:292)
	at kafka.log.Log$.apply(Log.scala:2158)
	at kafka.log.LogManager.loadLog(LogManager.scala:275)
	at kafka.log.LogManager.$anonfun$loadLogs$12(LogManager.scala:345)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:45:08,825] ERROR Error while deleting the clean shutdown file in dir /tmp/kafka-logs (kafka.server.LogDirFailureChannel)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.LogSegment.recover(LogSegment.scala:377)
	at kafka.log.Log.recoverSegment(Log.scala:500)
	at kafka.log.Log.$anonfun$loadSegmentFiles$3(Log.scala:482)
	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:792)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:791)
	at kafka.log.Log.loadSegmentFiles(Log.scala:454)
	at kafka.log.Log.$anonfun$loadSegments$1(Log.scala:565)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2024)
	at kafka.log.Log.loadSegments(Log.scala:559)
	at kafka.log.Log.<init>(Log.scala:292)
	at kafka.log.Log$.apply(Log.scala:2158)
	at kafka.log.LogManager.loadLog(LogManager.scala:275)
	at kafka.log.LogManager.$anonfun$loadLogs$12(LogManager.scala:345)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:45:08,827] ERROR Error while deleting the clean shutdown file in dir /tmp/kafka-logs (kafka.server.LogDirFailureChannel)
java.io.IOException: Invalid argument
	at java.base/java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:236)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:236)
	at kafka.log.LogSegment.recover(LogSegment.scala:377)
	at kafka.log.Log.recoverSegment(Log.scala:500)
	at kafka.log.Log.$anonfun$loadSegmentFiles$3(Log.scala:482)
	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:792)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:791)
	at kafka.log.Log.loadSegmentFiles(Log.scala:454)
	at kafka.log.Log.$anonfun$loadSegments$1(Log.scala:565)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2024)
	at kafka.log.Log.loadSegments(Log.scala:559)
	at kafka.log.Log.<init>(Log.scala:292)
	at kafka.log.Log$.apply(Log.scala:2158)
	at kafka.log.LogManager.loadLog(LogManager.scala:275)
	at kafka.log.LogManager.$anonfun$loadLogs$12(LogManager.scala:345)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-29 14:45:08,830] INFO Logs loading complete in 929 ms. (kafka.log.LogManager)
[2019-08-29 14:45:08,838] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-08-29 14:45:08,839] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-08-29 14:45:09,093] INFO Awaiting socket connections on slocalhost:9092. (kafka.network.Acceptor)
[2019-08-29 14:45:09,118] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(localhost,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-08-29 14:45:09,120] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-08-29 14:45:09,141] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:45:09,143] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:45:09,145] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:45:09,146] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:45:09,157] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-08-29 14:45:09,158] INFO [ReplicaManager broker=0] Stopping serving replicas in dir /tmp/kafka-logs (kafka.server.ReplicaManager)
[2019-08-29 14:45:09,161] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set() (kafka.server.ReplicaFetcherManager)
[2019-08-29 14:45:09,162] INFO [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set() (kafka.server.ReplicaAlterLogDirsManager)
[2019-08-29 14:45:09,174] INFO [ReplicaManager broker=0] Broker 0 stopped fetcher for partitions  and stopped moving logs for partitions  because they are in the failed log directory /tmp/kafka-logs. (kafka.server.ReplicaManager)
[2019-08-29 14:45:09,175] INFO Stopping serving logs in dir /tmp/kafka-logs (kafka.log.LogManager)
[2019-08-29 14:45:09,178] ERROR Shutdown broker because all log dirs in /tmp/kafka-logs have failed (kafka.log.LogManager)
[2019-08-29 14:45:09,531] WARN Unable to read additional data from client sessionid 0x100000fd48e0001, likely client has closed socket (org.apache.zookeeper.server.NIOServerCnxn)
[2019-08-29 14:45:09,533] INFO Closed socket connection for client /127.0.0.1:58749 which had sessionid 0x100000fd48e0001 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-08-29 14:45:15,066] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-08-29 14:45:15,357] INFO Expiring session 0x100000fd48e0001, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:45:15,360] INFO Processed session termination for sessionid: 0x100000fd48e0001 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:15,454] INFO starting (kafka.server.KafkaServer)
[2019-08-29 14:45:15,455] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-08-29 14:45:15,473] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:45:15,482] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:15,482] INFO Client environment:host.name=Christian-Inspiron.localdomain (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:15,482] INFO Client environment:java.version=11.0.4 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:15,483] INFO Client environment:java.vendor=Ubuntu (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:15,483] INFO Client environment:java.home=/usr/lib/jvm/java-11-openjdk-amd64 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:15,485] INFO Client environment:java.class.path=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/activation-1.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/aopalliance-repackaged-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/argparse4j-0.7.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/audience-annotations-0.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/commons-lang3-3.8.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-api-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-basic-auth-extension-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-file-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-json-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-runtime-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/connect-transforms-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/guava-20.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-api-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-locator-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/hk2-utils-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-core-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-databind-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-datatype-jdk8-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-base-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-jaxrs-json-provider-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jackson-module-jaxb-annotations-2.9.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javassist-3.22.0-CR2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.annotation-api-1.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.inject-2.5.0-b42.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.servlet-api-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/javax.ws.rs-api-2.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jaxb-api-2.3.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-client-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-common-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-container-servlet-core-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-hk2-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-media-jaxb-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jersey-server-2.27.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-client-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-continuation-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-http-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-io-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-security-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-server-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlet-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-servlets-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jetty-util-9.4.14.v20181114.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/jopt-simple-5.0.4.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-clients-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-log4j-appender-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-examples-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-scala_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-streams-test-utils-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka-tools-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0-sources.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/kafka_2.12-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/log4j-1.2.17.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/lz4-java-1.5.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/maven-artifact-3.6.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/metrics-core-2.2.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/osgi-resource-locator-1.0.1.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/plexus-utils-3.1.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/reflections-0.9.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/rocksdbjni-5.15.10.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-library-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-logging_2.12-3.9.0.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/scala-reflect-2.12.8.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-api-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/slf4j-log4j12-1.7.25.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/snappy-java-1.1.7.2.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/validation-api-1.1.0.Final.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zkclient-0.11.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zookeeper-3.4.13.jar:/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1/kafka_2.12-2.2.0/bin/../libs/zstd-jni-1.3.8-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:15,487] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:15,488] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:15,488] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:15,488] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:15,488] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:15,488] INFO Client environment:os.version=4.4.0-18362-Microsoft (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:15,488] INFO Client environment:user.name=christian (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:15,488] INFO Client environment:user.home=/home/christian (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:15,489] INFO Client environment:user.dir=/mnt/c/Users/pisch/Documents/GitHub/g3-serp2019/sem1 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:15,490] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@50378a4 (org.apache.zookeeper.ZooKeeper)
[2019-08-29 14:45:15,507] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:45:15,508] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:45:15,519] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:45:15,519] INFO Accepted socket connection from /127.0.0.1:58761 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-08-29 14:45:15,524] INFO Client attempting to establish new session at /127.0.0.1:58761 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:45:15,533] INFO Established session 0x100000fd48e0002 with negotiated timeout 6000 for client /127.0.0.1:58761 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-08-29 14:45:15,536] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100000fd48e0002, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:45:15,539] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:45:15,581] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0002 type:create cxid:0x1 zxid:0xc89 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:15,590] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0002 type:create cxid:0x2 zxid:0xc8a txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:15,595] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0002 type:create cxid:0x3 zxid:0xc8b txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:15,601] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0002 type:create cxid:0x4 zxid:0xc8c txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:15,604] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0002 type:create cxid:0x5 zxid:0xc8d txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:15,607] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0002 type:create cxid:0x6 zxid:0xc8e txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:15,610] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0002 type:create cxid:0x7 zxid:0xc8f txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:15,619] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0002 type:create cxid:0x8 zxid:0xc90 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:15,624] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0002 type:create cxid:0x9 zxid:0xc91 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:15,629] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0002 type:create cxid:0xa zxid:0xc92 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:15,634] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0002 type:create cxid:0xb zxid:0xc93 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:15,637] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0002 type:create cxid:0xc zxid:0xc94 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:15,640] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0002 type:create cxid:0xd zxid:0xc95 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:15,774] INFO Cluster ID = eSotZ3p7R9WkH6FL3yanHA (kafka.server.KafkaServer)
[2019-08-29 14:45:15,778] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-08-29 14:45:15,827] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-08-29 14:45:15,852] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-08-29 14:45:15,892] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:45:15,892] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:45:15,897] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-08-29 14:45:15,930] INFO Log directory /tmp/kafka-logs not found, creating it. (kafka.log.LogManager)
[2019-08-29 14:45:15,945] INFO Loading logs. (kafka.log.LogManager)
[2019-08-29 14:45:15,958] INFO Logs loading complete in 13 ms. (kafka.log.LogManager)
[2019-08-29 14:45:15,976] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-08-29 14:45:15,983] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-08-29 14:45:16,325] INFO Awaiting socket connections on slocalhost:9092. (kafka.network.Acceptor)
[2019-08-29 14:45:16,356] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(localhost,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-08-29 14:45:16,358] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-08-29 14:45:16,388] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:45:16,390] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:45:16,391] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:45:16,393] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:45:16,423] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-08-29 14:45:16,487] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-08-29 14:45:16,511] INFO Stat of the created znode at /brokers/ids/0 is: 3222,3222,1567055716500,1567055716500,1,0,0,72057662028513282,188,0,3222
 (kafka.zk.KafkaZkClient)
[2019-08-29 14:45:16,517] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(localhost,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 3222 (kafka.zk.KafkaZkClient)
[2019-08-29 14:45:16,519] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-08-29 14:45:16,574] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:45:16,584] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:45:16,586] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-08-29 14:45:16,621] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:45:16,623] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:45:16,635] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 12 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:16,655] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:8000,blockEndProducerId:8999) by writing to Zk with path version 9 (kafka.coordinator.transaction.ProducerIdManager)
[2019-08-29 14:45:16,680] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-08-29 14:45:16,683] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-08-29 14:45:16,683] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-08-29 14:45:16,722] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-08-29 14:45:16,741] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2019-08-29 14:45:16,753] INFO Kafka version: 2.2.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-08-29 14:45:16,754] INFO Kafka commitId: 05fcfde8f69b0349 (org.apache.kafka.common.utils.AppInfoParser)
[2019-08-29 14:45:16,755] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2019-08-29 14:45:16,789] INFO Got user-level KeeperException when processing sessionid:0x100000fd48e0002 type:multi cxid:0x6b zxid:0xc99 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-08-29 14:45:16,841] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, alerts-0, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, tdn-systrk-0, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-08-29 14:45:16,884] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:16,890] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 34 ms (kafka.log.Log)
[2019-08-29 14:45:16,892] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:16,893] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2019-08-29 14:45:16,894] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:16,904] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:16,926] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:16,927] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:45:16,928] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:16,928] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2019-08-29 14:45:16,929] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:16,929] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:16,941] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:16,942] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:45:16,943] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:16,943] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2019-08-29 14:45:16,943] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:16,944] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:16,956] INFO [Log partition=alerts-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:16,958] INFO [Log partition=alerts-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:45:16,958] INFO Created log for partition alerts-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:16,959] INFO [Partition alerts-0 broker=0] No checkpointed highwatermark is found for partition alerts-0 (kafka.cluster.Partition)
[2019-08-29 14:45:16,959] INFO Replica loaded for partition alerts-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:16,960] INFO [Partition alerts-0 broker=0] alerts-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:16,972] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:16,973] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:45:16,973] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:16,974] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2019-08-29 14:45:16,974] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:16,974] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:16,988] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:16,989] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:45:16,990] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:16,990] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2019-08-29 14:45:16,991] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:16,991] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,012] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,015] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-08-29 14:45:17,017] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,018] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2019-08-29 14:45:17,018] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,018] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,033] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,034] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:45:17,035] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,036] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2019-08-29 14:45:17,036] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,036] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,053] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,054] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:45:17,055] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,055] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2019-08-29 14:45:17,056] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,056] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,071] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,072] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:45:17,072] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,073] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2019-08-29 14:45:17,073] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,073] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,086] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,087] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:45:17,088] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,088] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2019-08-29 14:45:17,088] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,089] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,108] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,111] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:45:17,111] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,112] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,112] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,112] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,127] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,128] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:45:17,128] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,129] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2019-08-29 14:45:17,129] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,129] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,150] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,152] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:45:17,153] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,154] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2019-08-29 14:45:17,154] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,154] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,168] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,169] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:45:17,170] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,171] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2019-08-29 14:45:17,171] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,171] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,190] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,192] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:45:17,193] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,195] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2019-08-29 14:45:17,195] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,195] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,215] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,217] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:45:17,218] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,219] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2019-08-29 14:45:17,219] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,219] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,239] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,241] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:45:17,242] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,243] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2019-08-29 14:45:17,243] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,243] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,262] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,266] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-08-29 14:45:17,267] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,267] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2019-08-29 14:45:17,267] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,267] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,292] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,298] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-08-29 14:45:17,301] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,311] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2019-08-29 14:45:17,313] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,314] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,341] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,343] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-08-29 14:45:17,346] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,349] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2019-08-29 14:45:17,350] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,356] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,382] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,389] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[2019-08-29 14:45:17,390] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,391] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2019-08-29 14:45:17,391] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,391] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,421] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,423] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-08-29 14:45:17,424] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,424] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2019-08-29 14:45:17,424] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,424] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,448] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,453] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-08-29 14:45:17,455] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,456] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2019-08-29 14:45:17,456] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,457] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,473] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,474] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:45:17,475] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,479] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2019-08-29 14:45:17,479] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,479] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,501] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,503] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:45:17,503] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,504] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2019-08-29 14:45:17,504] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,504] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,519] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,520] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:45:17,520] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,522] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2019-08-29 14:45:17,523] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,523] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,543] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,545] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:45:17,546] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,547] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2019-08-29 14:45:17,547] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,547] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,563] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,566] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:45:17,566] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,567] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2019-08-29 14:45:17,567] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,567] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,588] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,590] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:45:17,590] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,591] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2019-08-29 14:45:17,591] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,591] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,606] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,607] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:45:17,607] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,608] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2019-08-29 14:45:17,608] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,609] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,635] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,638] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:45:17,638] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,639] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2019-08-29 14:45:17,639] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,639] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,655] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,655] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:45:17,656] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,656] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2019-08-29 14:45:17,658] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,658] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,680] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,682] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-08-29 14:45:17,683] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,684] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2019-08-29 14:45:17,684] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,684] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,705] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,707] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:45:17,707] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,708] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2019-08-29 14:45:17,708] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,709] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,723] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,724] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:45:17,725] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,728] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2019-08-29 14:45:17,729] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,730] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,747] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,748] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-08-29 14:45:17,750] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,750] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2019-08-29 14:45:17,750] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,750] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,768] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,769] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:45:17,770] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,770] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2019-08-29 14:45:17,770] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,770] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,786] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,787] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-08-29 14:45:17,788] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,789] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2019-08-29 14:45:17,789] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,789] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,805] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,805] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:45:17,806] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,807] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2019-08-29 14:45:17,807] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,807] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,820] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,821] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:45:17,821] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,822] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2019-08-29 14:45:17,822] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,822] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,837] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,837] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:45:17,838] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,839] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2019-08-29 14:45:17,839] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,839] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,853] INFO [Log partition=tdn-systrk-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,854] INFO [Log partition=tdn-systrk-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:45:17,854] INFO Created log for partition tdn-systrk-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,855] INFO [Partition tdn-systrk-0 broker=0] No checkpointed highwatermark is found for partition tdn-systrk-0 (kafka.cluster.Partition)
[2019-08-29 14:45:17,855] INFO Replica loaded for partition tdn-systrk-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,855] INFO [Partition tdn-systrk-0 broker=0] tdn-systrk-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,874] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,876] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-08-29 14:45:17,876] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,877] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2019-08-29 14:45:17,877] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,877] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,890] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,891] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:45:17,892] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,892] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2019-08-29 14:45:17,892] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,892] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,912] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,914] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-08-29 14:45:17,915] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,916] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2019-08-29 14:45:17,916] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,916] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,928] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,929] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:45:17,929] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,929] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2019-08-29 14:45:17,930] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,930] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,941] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,942] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:45:17,943] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,944] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2019-08-29 14:45:17,944] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,944] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,957] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,957] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:45:17,958] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,958] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2019-08-29 14:45:17,958] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,959] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,970] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,971] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-08-29 14:45:17,972] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,972] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2019-08-29 14:45:17,972] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,973] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:17,986] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:17,987] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:45:17,987] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:17,988] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2019-08-29 14:45:17,988] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:17,988] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:18,001] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-08-29 14:45:18,002] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-08-29 14:45:18,002] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, segment.ms -> 604800000, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-08-29 14:45:18,003] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2019-08-29 14:45:18,003] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-08-29 14:45:18,003] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-08-29 14:45:18,023] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,025] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,025] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,026] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,026] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,026] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,026] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,027] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,027] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,027] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,028] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,028] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,028] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,028] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,028] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,029] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,029] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,029] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,029] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,029] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,029] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,030] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,030] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,030] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,030] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,030] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,031] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,031] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,031] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,031] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,031] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,031] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,032] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,032] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,032] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,032] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,033] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,037] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,037] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 12 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,037] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,038] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,038] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,038] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,038] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,038] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,038] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,038] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,039] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,039] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,039] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,039] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,039] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,039] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,039] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,039] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,040] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,040] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,040] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,040] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,040] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,040] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,041] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,041] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,041] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,041] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,042] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,042] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,042] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,042] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,042] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,043] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,043] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,043] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,043] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,044] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,044] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,044] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,044] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,045] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,045] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,045] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,045] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,045] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,046] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,050] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,051] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,051] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,051] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,051] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,052] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,052] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,052] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,052] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,052] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,053] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,053] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,053] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,053] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,053] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:45:18,054] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:49:28,871] INFO [GroupCoordinator 0]: Preparing to rebalance group console-consumer-48543 in state PreparingRebalance with old generation 0 (__consumer_offsets-35) (reason: Adding new member consumer-1-b63dd352-5c14-4787-ad08-90fb225a3f18) (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:49:28,881] INFO [GroupCoordinator 0]: Stabilized group console-consumer-48543 generation 1 (__consumer_offsets-35) (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:49:28,889] INFO [GroupCoordinator 0]: Assignment received from leader for group console-consumer-48543 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:55:16,628] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-08-29 14:56:25,268] INFO Unable to read additional data from server sessionid 0x100000fd48e0002, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:56:25,373] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:56:25,375] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:56:27,047] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:56:27,954] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:56:27,956] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2019-08-29 14:56:27,957] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)
[2019-08-29 14:56:29,053] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:56:29,156] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:56:29,158] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-08-29 14:56:30,199] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:56:32,202] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:56:33,445] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:56:35,449] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:56:36,419] INFO [GroupCoordinator 0]: Member consumer-1-b63dd352-5c14-4787-ad08-90fb225a3f18 in group console-consumer-48543 has left, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:56:36,420] INFO [GroupCoordinator 0]: Preparing to rebalance group console-consumer-48543 in state PreparingRebalance with old generation 1 (__consumer_offsets-35) (reason: removing member consumer-1-b63dd352-5c14-4787-ad08-90fb225a3f18 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:56:36,421] INFO [GroupCoordinator 0]: Group console-consumer-48543 with generation 2 is now empty (__consumer_offsets-35) (kafka.coordinator.group.GroupCoordinator)
[2019-08-29 14:56:36,863] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:56:38,871] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:56:40,456] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:56:42,460] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:56:42,553] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:56:44,452] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:56:46,457] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:56:47,761] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:56:49,386] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:56:49,568] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:56:49,737] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:56:49,770] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:56:49,901] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:56:50,066] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:56:50,255] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:56:51,658] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:56:53,663] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:13,338] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-08-29 14:57:13,341] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:15,344] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:17,002] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:19,004] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:20,359] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:22,366] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:23,612] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:25,617] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:26,864] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:28,870] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:30,140] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:32,144] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:33,255] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:35,259] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:37,110] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:39,115] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:40,426] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:42,433] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:43,857] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:45,862] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:47,114] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:49,120] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:51,130] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:53,135] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:55,000] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:57,003] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:57:58,176] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:00,179] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:01,765] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:03,768] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:04,934] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:06,940] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:08,118] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:10,122] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:11,724] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:13,731] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:15,680] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:17,684] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:19,395] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:21,403] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:22,819] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:24,824] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:26,356] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:28,359] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:29,507] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:31,510] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:33,592] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:35,594] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:37,349] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:39,354] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:41,006] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:43,009] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:44,559] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:46,563] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:48,353] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:50,357] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:51,607] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:53,612] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:55,364] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:57,367] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:58:58,950] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:00,954] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:02,854] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:04,857] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:06,359] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:08,365] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:10,256] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:12,259] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:14,142] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:16,155] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:17,393] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:19,399] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:21,478] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:23,481] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:25,167] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:27,171] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:28,463] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:30,467] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:32,114] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:34,122] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:35,337] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:37,342] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:38,693] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:40,700] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:42,565] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:44,568] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:46,433] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:48,438] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:50,346] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:52,349] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:53,823] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:55,828] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:57,490] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-08-29 14:59:59,494] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
